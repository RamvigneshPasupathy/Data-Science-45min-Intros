{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from tweet_parser.tweet import Tweet\n",
    "from gapi import gnipapi\n",
    "from gapi.gnipapi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will lean heavily on Tom Augspurger's excellent series on Modern Pandas.\n",
    "\n",
    "\n",
    "Quote:\n",
    "\n",
    "    Method chaining, where you call methods on an object one after another, is in vogue at the moment. It's always been a style of programming that's been possible with pandas, and over the past several releases, we've added methods that enable even more chaining.\n",
    "\n",
    "    - assign (0.16.0): For adding new columns to a DataFrame in a chain (inspired by dplyr's mutate)\n",
    "    - pipe (0.16.2): For including user-defined methods in method chains.\n",
    "    - rename (0.18.0): For altering axis names (in additional to changing the actual labels as before).\n",
    "\n",
    "    - Window methods (0.18): Took the top-level pd.rolling\\_\\* and pd.expanding\\_\\* functions and made them NDFrame methods with a groupby-like API.\n",
    "    - Resample (0.18.0) Added a new groupby-like API\n",
    "    - .where/mask/Indexers accept Callables (0.18.1): In the next release you'll be able to pass a callable to the indexing methods, to be evaluated within the DataFrame's context (like .query, but with code instead of strings).\n",
    "\n",
    "    My scripts will typically start off with large-ish chain at the start getting things into a manageable state. It's good to have the bulk of your munging done with right away so you can start to do Scienceâ„¢:\n",
    "    \n",
    "    \n",
    "Part of the goal will be to develop different coding styles with Pandas, moving from a script-ish, verbose approach to a piped style that flows well with discrete cleaning operations grouped into single functions. This flows very well into using pyspark's dataframe as well, as pyspark *requires* that kind of style and there is a great deal of overlap with pandas' dataframe methods in pyspark.   \n",
    "\n",
    "\n",
    "Method chains are a popular method in programming these days, with the rise of functional languages that can change function composition to be more readable. Examples of this in various languages:\n",
    "\n",
    "```.scala\n",
    " def fooNotIndent : List[Int] = (1 to 100).view.map { _ + 3 }.filter { _ > 10 }.flatMap { table.get }.take(3).toList\n",
    "\n",
    "  def fooIndent: List[Int] =\n",
    "  (1 to 100)\n",
    "    .view\n",
    "    .map { _ + 3 }\n",
    "    .filter { _ > 10 }\n",
    "    .flatMap { table.get }\n",
    "    .take(3)\n",
    "    .toList\n",
    "```\n",
    "\n",
    "\n",
    "or comparing (from TA's post)\n",
    "\n",
    "\n",
    "```\n",
    "tumble_after(\n",
    "    broke(\n",
    "        fell_down(\n",
    "            fetch(went_up(jack_jill, \"hill\"), \"water\"),\n",
    "            jack),\n",
    "        \"crown\"),\n",
    "    \"jill\"\n",
    ")\n",
    "```\n",
    "\n",
    "with (from TA's post)\n",
    "\n",
    "```\n",
    "jack_jill %>%\n",
    "    went_up(\"hill\") %>%\n",
    "    fetch(\"water\") %>%\n",
    "    fell_down(\"jack\") %>%\n",
    "    broke(\"crown\") %>%\n",
    "    tumble_after(\"jill\")\n",
    "    \n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "jack_jill \n",
    "    .pipe(went(\"hill\", \"up\"))\n",
    "    .pipe(fetch(\"water\"))\n",
    "    .pipe(fell_down(\"jack\"))\n",
    "    .pipe(broke(\"crown\"))\n",
    "    .pipe(tumble_after(\"jill\"))\n",
    "``` \n",
    "\n",
    "\n",
    "\n",
    "There are several cases I'd like to address in this session -\n",
    "\n",
    "1. Effective pandas usage\n",
    "2. Interactive development strategies\n",
    "3. Balancing exploration and reproducability\n",
    "4. Joining heterogenous datatypes\n",
    "\n",
    "\n",
    "This might be a lot for a single session, but hey. \n",
    "\n",
    "Let's start off with a problem that we might have that we can try to answer:\n",
    "\n",
    "\n",
    "## What Airports or flights have issues with delays?\n",
    "\n",
    "I am purposefully choosing a dataset that we will have difficulties in joining with twitter data, and also to illustrate a point...\n",
    "\n",
    "Let's use Tom's use of the BTS airline delay dataset, which requires a bit of work to obtain and parse through.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data work, I have a loose set of semantics to describe stages in a workflow:\n",
    "\n",
    "### Research and understanding\n",
    "- understand question of interest\n",
    "- understand the data sources available\n",
    "- evaluate requirements from stakeholders\n",
    "- think about available methods and timeframes for implementation\n",
    "- judge final output (serialized data, production model, figures / slides / report, etc)\n",
    "\n",
    "    \n",
    "### data gathering / pull\n",
    "- a single stage of collecting data from some source, be it scraping a website, some form of database, reading from a csv or other binary file, etc.\n",
    "    \n",
    "    \n",
    "### trivial cleaning\n",
    "- dealing with various data sources returns data in formats that you may not want, be it with weird variable names, transformations from CamelCase to camel_case, or other very early-stages ops that you define to ease the rest of the process. E.g., renaming columns with spaces in them to be `_` delinated. This can often be integrated into the basic data pull step, but should be explicit for reproducibility purposes.\n",
    "    \n",
    "    \n",
    "### Non-trivial cleaning / preprocessing\n",
    "- There might be missing data, multiple or non-standard representation of NULL values (99s, strings 'nan', etc), which finding and handling are crucial\n",
    "- for JSON or similar formats, nested data structures might be present\n",
    "- datetime parsing and conversion if important to underlying analysis\n",
    "- reshaping data to facilitate analysis (wide to long, stacked to unstacked, etc).\n",
    "- frequency normalization for time-series data\n",
    "- text cleaning or tokenization\n",
    "- joining additional data sources with current data (which has already be gathered and pulled)\n",
    "\n",
    "\n",
    "### exploratory analysis\n",
    "- with \"clean\" data, you can begin to poke at questions, from basic summarization and counting to faceted charts if it makes sense for the data\n",
    "- may include branching by reformatting your data into a different set (single operations to aggregated buckets, denormalized time-series stats, etc)\n",
    "- lots of plotting and descriptions\n",
    "- often can loop back to previous stages to gather more data or stabilize workflow when finding good features or ways of processing data\n",
    "\n",
    "\n",
    "### modeling / prediction\n",
    "- potential input to ML functions, which includes sampling and so forth\n",
    "- output could be predicted values for a database or downstream operation, figures and text for a report, etc.\n",
    "    \n",
    "   \n",
    "   \n",
    "It's important to note that these stages are NEVER LINEAR, even though it almost always looks like it to the end consumer of posts like this. Each stage can be non-trivial for a host of reasons, and choices made in the early stages have strong effects in the rest of the process. Lots of iteration might be needed in each stage, and managing technical debt here can make each iteration faster.\n",
    "\n",
    "\n",
    "Given these tasks, it seems logical to define our code in simliar stages, though I have no precise guides for how to do this. In our example for today, we can start by grabbing some data from the web. We'll follow TA's flights data grab and later add some tweet data via the Gnip api.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play\n",
    "\n",
    "What type of questions do we want to answer? Let's say we have a project that will investigate customer sentiment around airports / airlines. Perhaps some questions of interest are:\n",
    "\n",
    "- Are customer moods affected heavily by flight delays?\n",
    "- Are customers more likely to tweet due to flight delays?\n",
    "- Do these effects vary by airport, airlines, or other factors?\n",
    "\n",
    "\n",
    "What type of data will we need? Probably some detailed data about flight delays, preferabbly flight-level data that includes information about the carrier, airport, destination, etc. We'll also need tweet data that reasonably matches these critera, of course. In this context, we'll probably be satisfied with simple exploration.\n",
    "\n",
    "I chose these questions partially due to the great series of posts by Augsperger that illustrate working with complex data, such as flight delays. :) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "For most notebook purposes, I like to include all imports at the top of a notebook, even if the code they enable will not be introduced until a later point. I like to keep this cell apart from others in the notebook for easy maintenence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original example from Tom, the code is written out as such:\n",
    "\n",
    "```.python\n",
    "headers = {\n",
    "    'Referer': 'https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time',\n",
    "    'Origin': 'https://www.transtats.bts.gov',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "}\n",
    "\n",
    "params = (\n",
    "    ('Table_ID', '236'),\n",
    "    ('Has_Group', '3'),\n",
    "    ('Is_Zipped', '0'),\n",
    ")\n",
    "\n",
    "data = <TRUNCATED>\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "dest = \"data/flights.csv.zip\"\n",
    "\n",
    "if not os.path.exists(dest):\n",
    "    r = requests.post('https://www.transtats.bts.gov/DownLoad_Table.asp',\n",
    "                      headers=headers, params=params, data=data, stream=True)\n",
    "\n",
    "    with open(\"data/flights.csv.zip\", 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=102400): \n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "```\n",
    "\n",
    "\n",
    "Given out focus today, let's wrap all initial data pulling into a function for logical separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_pull_airport_data():\n",
    "    \"\"\"\n",
    "    lightly modified from TA's post.\n",
    "    \n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Referer': 'https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time',\n",
    "        'Origin': 'https://www.transtats.bts.gov',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('Table_ID', '236'),\n",
    "        ('Has_Group', '3'),\n",
    "        ('Is_Zipped', '0'),\n",
    "    )\n",
    "    \n",
    "    # query string to be sent. can modify the 'where' dates to change the size of data returned.\n",
    "\n",
    "    data = \"UserTableName=On_Time_Performance&DBShortName=On_Time&RawDataTable=T_ONTIME&sqlstr=+SELECT+FL_DATE%2CUNIQUE_CARRIER%2CAIRLINE_ID%2CTAIL_NUM%2CFL_NUM%2CORIGIN_AIRPORT_ID%2CORIGIN_AIRPORT_SEQ_ID%2CORIGIN_CITY_MARKET_ID%2CORIGIN%2CORIGIN_CITY_NAME%2CDEST_AIRPORT_ID%2CDEST_AIRPORT_SEQ_ID%2CDEST_CITY_MARKET_ID%2CDEST%2CDEST_CITY_NAME%2CCRS_DEP_TIME%2CDEP_TIME%2CDEP_DELAY%2CTAXI_OUT%2CWHEELS_OFF%2CWHEELS_ON%2CTAXI_IN%2CCRS_ARR_TIME%2CARR_TIME%2CARR_DELAY%2CCANCELLED%2CCANCELLATION_CODE%2CCARRIER_DELAY%2CWEATHER_DELAY%2CNAS_DELAY%2CSECURITY_DELAY%2CLATE_AIRCRAFT_DELAY+FROM++T_ONTIME+WHERE+YEAR%3D2017&varlist=FL_DATE%2CUNIQUE_CARRIER%2CAIRLINE_ID%2CTAIL_NUM%2CFL_NUM%2CORIGIN_AIRPORT_ID%2CORIGIN_AIRPORT_SEQ_ID%2CORIGIN_CITY_MARKET_ID%2CORIGIN%2CORIGIN_CITY_NAME%2CDEST_AIRPORT_ID%2CDEST_AIRPORT_SEQ_ID%2CDEST_CITY_MARKET_ID%2CDEST%2CDEST_CITY_NAME%2CCRS_DEP_TIME%2CDEP_TIME%2CDEP_DELAY%2CTAXI_OUT%2CWHEELS_OFF%2CWHEELS_ON%2CTAXI_IN%2CCRS_ARR_TIME%2CARR_TIME%2CARR_DELAY%2CCANCELLED%2CCANCELLATION_CODE%2CCARRIER_DELAY%2CWEATHER_DELAY%2CNAS_DELAY%2CSECURITY_DELAY%2CLATE_AIRCRAFT_DELAY&grouplist=&suml=&sumRegion=&filter1=title%3D&filter2=title%3D&geo=All%A0&time=January&timename=Month&GEOGRAPHY=All&XYEAR=2017&FREQUENCY=1&VarDesc=Year&VarType=Num&VarDesc=Quarter&VarType=Num&VarDesc=Month&VarType=Num&VarDesc=DayofMonth&VarType=Num&VarDesc=DayOfWeek&VarType=Num&VarName=FL_DATE&VarDesc=FlightDate&VarType=Char&VarName=UNIQUE_CARRIER&VarDesc=UniqueCarrier&VarType=Char&VarName=AIRLINE_ID&VarDesc=AirlineID&VarType=Num&VarDesc=Carrier&VarType=Char&VarName=TAIL_NUM&VarDesc=TailNum&VarType=Char&VarName=FL_NUM&VarDesc=FlightNum&VarType=Char&VarName=ORIGIN_AIRPORT_ID&VarDesc=OriginAirportID&VarType=Num&VarName=ORIGIN_AIRPORT_SEQ_ID&VarDesc=OriginAirportSeqID&VarType=Num&VarName=ORIGIN_CITY_MARKET_ID&VarDesc=OriginCityMarketID&VarType=Num&VarName=ORIGIN&VarDesc=Origin&VarType=Char&VarName=ORIGIN_CITY_NAME&VarDesc=OriginCityName&VarType=Char&VarDesc=OriginState&VarType=Char&VarDesc=OriginStateFips&VarType=Char&VarDesc=OriginStateName&VarType=Char&VarDesc=OriginWac&VarType=Num&VarName=DEST_AIRPORT_ID&VarDesc=DestAirportID&VarType=Num&VarName=DEST_AIRPORT_SEQ_ID&VarDesc=DestAirportSeqID&VarType=Num&VarName=DEST_CITY_MARKET_ID&VarDesc=DestCityMarketID&VarType=Num&VarName=DEST&VarDesc=Dest&VarType=Char&VarName=DEST_CITY_NAME&VarDesc=DestCityName&VarType=Char&VarDesc=DestState&VarType=Char&VarDesc=DestStateFips&VarType=Char&VarDesc=DestStateName&VarType=Char&VarDesc=DestWac&VarType=Num&VarName=CRS_DEP_TIME&VarDesc=CRSDepTime&VarType=Char&VarName=DEP_TIME&VarDesc=DepTime&VarType=Char&VarName=DEP_DELAY&VarDesc=DepDelay&VarType=Num&VarDesc=DepDelayMinutes&VarType=Num&VarDesc=DepDel15&VarType=Num&VarDesc=DepartureDelayGroups&VarType=Num&VarDesc=DepTimeBlk&VarType=Char&VarName=TAXI_OUT&VarDesc=TaxiOut&VarType=Num&VarName=WHEELS_OFF&VarDesc=WheelsOff&VarType=Char&VarName=WHEELS_ON&VarDesc=WheelsOn&VarType=Char&VarName=TAXI_IN&VarDesc=TaxiIn&VarType=Num&VarName=CRS_ARR_TIME&VarDesc=CRSArrTime&VarType=Char&VarName=ARR_TIME&VarDesc=ArrTime&VarType=Char&VarName=ARR_DELAY&VarDesc=ArrDelay&VarType=Num&VarDesc=ArrDelayMinutes&VarType=Num&VarDesc=ArrDel15&VarType=Num&VarDesc=ArrivalDelayGroups&VarType=Num&VarDesc=ArrTimeBlk&VarType=Char&VarName=CANCELLED&VarDesc=Cancelled&VarType=Num&VarName=CANCELLATION_CODE&VarDesc=CancellationCode&VarType=Char&VarDesc=Diverted&VarType=Num&VarDesc=CRSElapsedTime&VarType=Num&VarDesc=ActualElapsedTime&VarType=Num&VarDesc=AirTime&VarType=Num&VarDesc=Flights&VarType=Num&VarDesc=Distance&VarType=Num&VarDesc=DistanceGroup&VarType=Num&VarName=CARRIER_DELAY&VarDesc=CarrierDelay&VarType=Num&VarName=WEATHER_DELAY&VarDesc=WeatherDelay&VarType=Num&VarName=NAS_DELAY&VarDesc=NASDelay&VarType=Num&VarName=SECURITY_DELAY&VarDesc=SecurityDelay&VarType=Num&VarName=LATE_AIRCRAFT_DELAY&VarDesc=LateAircraftDelay&VarType=Num&VarDesc=FirstDepTime&VarType=Char&VarDesc=TotalAddGTime&VarType=Num&VarDesc=LongestAddGTime&VarType=Num&VarDesc=DivAirportLandings&VarType=Num&VarDesc=DivReachedDest&VarType=Num&VarDesc=DivActualElapsedTime&VarType=Num&VarDesc=DivArrDelay&VarType=Num&VarDesc=DivDistance&VarType=Num&VarDesc=Div1Airport&VarType=Char&VarDesc=Div1AirportID&VarType=Num&VarDesc=Div1AirportSeqID&VarType=Num&VarDesc=Div1WheelsOn&VarType=Char&VarDesc=Div1TotalGTime&VarType=Num&VarDesc=Div1LongestGTime&VarType=Num&VarDesc=Div1WheelsOff&VarType=Char&VarDesc=Div1TailNum&VarType=Char&VarDesc=Div2Airport&VarType=Char&VarDesc=Div2AirportID&VarType=Num&VarDesc=Div2AirportSeqID&VarType=Num&VarDesc=Div2WheelsOn&VarType=Char&VarDesc=Div2TotalGTime&VarType=Num&VarDesc=Div2LongestGTime&VarType=Num&VarDesc=Div2WheelsOff&VarType=Char&VarDesc=Div2TailNum&VarType=Char&VarDesc=Div3Airport&VarType=Char&VarDesc=Div3AirportID&VarType=Num&VarDesc=Div3AirportSeqID&VarType=Num&VarDesc=Div3WheelsOn&VarType=Char&VarDesc=Div3TotalGTime&VarType=Num&VarDesc=Div3LongestGTime&VarType=Num&VarDesc=Div3WheelsOff&VarType=Char&VarDesc=Div3TailNum&VarType=Char&VarDesc=Div4Airport&VarType=Char&VarDesc=Div4AirportID&VarType=Num&VarDesc=Div4AirportSeqID&VarType=Num&VarDesc=Div4WheelsOn&VarType=Char&VarDesc=Div4TotalGTime&VarType=Num&VarDesc=Div4LongestGTime&VarType=Num&VarDesc=Div4WheelsOff&VarType=Char&VarDesc=Div4TailNum&VarType=Char&VarDesc=Div5Airport&VarType=Char&VarDesc=Div5AirportID&VarType=Num&VarDesc=Div5AirportSeqID&VarType=Num&VarDesc=Div5WheelsOn&VarType=Char&VarDesc=Div5TotalGTime&VarType=Num&VarDesc=Div5LongestGTime&VarType=Num&VarDesc=Div5WheelsOff&VarType=Char&VarDesc=Div5TailNum&VarType=Char\"\n",
    "\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    dest = \"data/flights.csv.zip\"\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        r = requests.post('https://www.transtats.bts.gov/DownLoad_Table.asp',\n",
    "                          headers=headers, params=params, data=data, stream=True)\n",
    "\n",
    "        with open(\"data/flights.csv.zip\", 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=102400): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    \n",
    "\n",
    "    zf = zipfile.ZipFile(\"data/flights.csv.zip\")\n",
    "    fp = zf.extract(zf.filelist[0].filename, path='data/')\n",
    "    df = (pd\n",
    "          .read_csv(fp, parse_dates=[\"FL_DATE\"])\n",
    "          .rename(columns=str.lower) #note this takes a callable\n",
    "         )\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function may be a bit sloppy from a DRY standpoint, but let's be serious: there is no need for arguments in this function and no other piece of our analysis will ever touch the fields inside of here. You could argue that it could take a flexibile filename option, but again, for the purposes of this demo, that might be overkill, but refactoring the single function to take a filename argument would take a minutue or two, now that the core logic is stable. This gives us a high-level intro point for our demo, a single call to the function.\n",
    "\n",
    "Imagine this was going to go into a much larger set of functions or a library of some sort -- the function can be moved to a python file and work out of the box, which can simply your notebook or code at the risk of making more dependencies for users and disrupting the flow of analysis for a technical consumer.\n",
    "\n",
    "\n",
    "That was a lot of crap - let's get back to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = maybe_pull_airport_data()\n",
    "\n",
    "flights.head()\n",
    "flights.shape\n",
    "flights.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression on pandas\n",
    "Woo, we have a moderate-sized dataframe with a lot of columns, many which are NAN or non-intuitive. Let's define a few indices on the data, which assign metadata to each row and allow for fancy selection and operations along the way.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = flights.set_index([\"unique_carrier\", \"origin\", \"dest\", \"tail_num\", \"fl_date\"]).sort_index()\n",
    "hdf[hdf.columns[:4]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this clears up some thinking about the rows -- indexing by the flight operator, the airport origin, airport destination, the plan id, and the date of the flight make it clear what each row is.\n",
    "\n",
    "Selecting the data out in useful ways is somewhat straightforward, using the `.loc` semantics, which allow for *label-oriented indexing* in a dataframe.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.loc[[\"AA\"], [\"dep_delay\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to get ANY flight from denver to albuquerque? Pandas `IndexSlice` is a brilliant help here.\n",
    "\n",
    "The semantics work as follows:\n",
    "\n",
    "`:` is \"include all labels from this level of the index\n",
    "\n",
    "`hdf.loc[pd.IndexSlice[:, [\"DEN\"], [\"ABQ\"]], [\"dep_delay\"]]`\n",
    "\n",
    "translates to\n",
    "\n",
    "`hdf.loc[pd.IndexSlice[ALL CARRIERS, origin=[\"DEN\"], dest=[\"ABQ\"], ALL_TAILS, ALL_DATES], [\"dep_delay\"]]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hdf.loc[pd.IndexSlice[:, [\"DEN\"], [\"ABQ\"]],\n",
    "         [\"dep_delay\"]]\n",
    " .sort_values(\"dep_delay\", ascending=False)\n",
    " .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also use the powerful `query` function, which allows a limited vocabulary to be executed on a dataframe and is wildly useful for slightly more clear operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hdf\n",
    " .query(\"origin == 'DEN' and dest == 'ABQ'\")\n",
    " .loc[:, \"dep_delay\"]\n",
    " .to_frame()\n",
    " .sort_values(\"dep_delay\", ascending=False)\n",
    " .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These days, I prefer `query` most of the time, particularly for exploration, but `.loc` with explicit indices can be far faster in many cases.\n",
    "\n",
    "### back to gathering and inspection\n",
    "\n",
    "So, at this stage, it seems reasonable to examine some of the data and see what might be problematic or needs further work.\n",
    "\n",
    "We can see that several columns that should be datetimes are not \"dep_time\", etc. and that the City names are not really city names but City, State pairs. In Toms' series, the cleaning operations are done in a different post, but I will copy them here to fit in our framework. I think these functions can be safelycounted as advanced preprocessing and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.head()\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_city_name(df):\n",
    "    '''\n",
    "    Chicago, IL -> Chicago for origin_city_name and dest_city_name. From Augsperger.\n",
    "    '''\n",
    "    cols = ['origin_city_name', 'dest_city_name']\n",
    "    city = df[cols].apply(lambda x: x.str.extract(\"(.*), \\w{2}\", expand=False))\n",
    "    df = df.copy()\n",
    "    df[['origin_city_name', 'dest_city_name']] = city\n",
    "    return df\n",
    "\n",
    "\n",
    "def time_to_datetime(df, columns):\n",
    "    '''\n",
    "    Combine all time items into datetimes. From Augsperger.\n",
    "\n",
    "    2014-01-01,0914 -> 2014-01-01 09:14:00\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    def converter(col):\n",
    "        timepart = (col.astype(str)\n",
    "                       .str.replace('\\.0$', '')  # NaNs force float dtype\n",
    "                       .str.pad(4, fillchar='0'))\n",
    "        return pd.to_datetime(df['fl_date'].astype(\"str\") + ' ' +\n",
    "                               timepart.str.slice(0, 2) + ':' +\n",
    "                               timepart.str.slice(2, 4),\n",
    "                               errors='coerce')\n",
    "    df[columns] = df[columns].apply(converter)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both methods accept a `pandas.DataFrame` and return a `pandas.DataFrame` . This is critical to our upcoming methodology, and for portability to spark.\n",
    "\n",
    "It seems obvious, but writing code that operates on immutable data structures is wildly useful for data processing. DataFrames are not immutable, but can be treated as such, as many operations either implicitly return a copy or methods can be written as such. With our methods, we can now create a new top-level function that handles our preprocessing.\n",
    "\n",
    "It's not too often that your major performance bottleneck in pandas is copying dataframes.\n",
    "\n",
    "Anyway, we can now integrate our simple gathering method with some of the cleaning methods for a new top-level entry for our exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_process_flights_data():\n",
    "    drop_cols = [\"unnamed: 32\", \"security_delay\", \"late_aircraft_delay\",\n",
    "                 \"nas_delay\", \"origin_airport_id\", \"origin_city_market_id\",\n",
    "                 \"taxi_out\", \"wheels_off\", \"wheels_on\", \"crs_arr_time\", \"crs_dep_time\",\n",
    "                 \"carrier_delay\"]\n",
    "    df = (maybe_pull_airport_data()\n",
    "          .rename(columns=str.lower)\n",
    "          .drop(drop_cols, axis=1)\n",
    "          .pipe(extract_city_name)\n",
    "          .pipe(time_to_datetime, ['dep_time', 'arr_time'])\n",
    "          .assign(fl_date=lambda x: pd.to_datetime(x['fl_date']),\n",
    "                  dest=lambda x: pd.Categorical(x['dest']),\n",
    "                  origin=lambda x: pd.Categorical(x['origin']),\n",
    "                  tail_num=lambda x: pd.Categorical(x['tail_num']),\n",
    "                  unique_carrier=lambda x: pd.Categorical(x['unique_carrier']),\n",
    "                  cancellation_code=lambda x: pd.Categorical(x['cancellation_code'])))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this will take a few minutues with the full 2017 data; far faster with a month's sample\n",
    "flights = read_and_process_flights_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.tail()\n",
    "flights.dtypes\n",
    "flights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data\n",
    "\n",
    "\n",
    "From Tom's post, here's a long method chain that does an awful lot of work to generate a plot of flights per day for the top carriers. I'll break this down a bit after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(flights\n",
    " .dropna(subset=['dep_time', 'unique_carrier'])\n",
    " .loc[flights['unique_carrier'].isin(flights['unique_carrier']\n",
    "                                .value_counts()\n",
    "                                .index[:5])]\n",
    " .set_index('dep_time')\n",
    " .groupby(['unique_carrier', pd.TimeGrouper(\"D\")])\n",
    " [\"fl_num\"]\n",
    " .count()\n",
    " .unstack(0)\n",
    " .fillna(0)\n",
    " .rename_axis(\"Flights per Day\", axis=1)\n",
    " .plot()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we broke this out like many people do, we might end up with code like this, where each step is broken into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the carriers with the most traffic, hacking with the index. We use this for other ops. \n",
    "df_clean = flights.dropna(subset=[\"dep_time\", \"unique_carrier\"])\n",
    "top_carriers = flights[\"unique_carrier\"].value_counts().index[:5]\n",
    "df_clean = df_clean.query(\"unique_carrier in @top_carriers\")\n",
    "df_clean = df_clean.set_index(\"dep_time\")\n",
    "\n",
    "carriers_by_hour = (df_clean\n",
    "                    .groupby(['unique_carrier',\n",
    "                              pd.TimeGrouper(\"H\")])[\"fl_num\"]\n",
    "                    .count())\n",
    "carriers_df = carriers_by_hour.unstack(0)\n",
    "carriers_df = carriers_df.fillna(0)\n",
    "carriers_flights_per_day = (carriers_df\n",
    "                            .rolling(24)\n",
    "                            .sum()\n",
    "                            .rename_axis(\"Flights per Day\", axis=1))\n",
    "\n",
    "carriers_flights_per_day.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming things is hard. Given that pandas has exteremely expressive semantics and nearly all analytic methods return a fresh dataframe or series, it makes it straightforward to chain many ops together. This style will lend itself well to spark and should be familiar to those of you who have worked with Scala or other functional languages.\n",
    "\n",
    "If the chains get very verbose or hard to follow, break them up and put them in a function, where you can keep it all in one place. Try to be very specific about naming your functions (remember, naming things is hard, functions are no different).\n",
    "\n",
    "\n",
    "In an exploratory context, you might continue adding methods onto your chain until you can expand and continue until you get to your chart or end stage goal. In some cases, saving some exploratory work to varibles is great. \n",
    "\n",
    "\n",
    "Let's briefly talk about the `.assign` operator. This operation returns a new column for a dataframe, where the new column can be a constant, some like-indexed numpy array or series, a callable that references the dataframe in question, etc. It's very powerful in method chains and also very useful for keeping your namespace clean.\n",
    "\n",
    "the semantics of \n",
    "\n",
    "`df.assign(NEW_COLUMN_NAME=lambda df: df[\"column\"] + df[\"column2\"]`\n",
    "can be read as\n",
    "\n",
    "assign a column named \"NEW_COLUMN_NAME\" to my referenced dataframe that is the sum of \"column\" and \"column2\". In the below example , the lambda references the datetime object of the departure time column to extract the hour, which gives us a convenient categorical value for examination.\n",
    "\n",
    "\n",
    "This is similar to R's `mutate` function in the dyplr world.\n",
    "\n",
    "\n",
    "Note -- the `top_carriers` variable above is a good example of something we might want to keep around, and I'll use it several times in the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from Augsperger\n",
    "(flights[['fl_date', 'unique_carrier', 'tail_num', 'dep_time', 'dep_delay']]\n",
    " .dropna()\n",
    " .query(\"unique_carrier in @top_carriers\")\n",
    " .assign(hour=lambda x: x['dep_time'].dt.hour)\n",
    " #.query('5 <= dep_delay < 600')\n",
    " .pipe((sns.boxplot, 'data'), 'hour', 'dep_delay')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables rapid exploration, and within the interactive context, allows you to copy a cell and change single lines to modify your results. \n",
    "\n",
    "A heatmap might be a nice way to visualize categories in this data, and the `assign` syntax allows creating those categoricals seamless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(flights[['fl_date', \"unique_carrier\", 'dep_time', 'dep_delay']]\n",
    " .dropna()\n",
    " .query(\"unique_carrier in @top_carriers\")\n",
    " .assign(hour=lambda x: x.dep_time.dt.hour)\n",
    " .assign(day=lambda x: x.dep_time.dt.dayofweek)\n",
    " .query('-1 < dep_delay < 600')\n",
    " .groupby([\"day\", \"hour\"])[\"dep_delay\"]\n",
    " .median()\n",
    " .unstack()\n",
    " .pipe((sns.heatmap, 'data'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(flights[['fl_date', 'unique_carrier', 'dep_time', 'dep_delay']]\n",
    " .query(\"unique_carrier in @top_carriers\")\n",
    " .dropna()\n",
    " .assign(hour=lambda x: x.dep_time.dt.hour)\n",
    " .assign(day=lambda x: x.dep_time.dt.dayofweek)\n",
    " #.query('0 <= dep_delay < 600')\n",
    " .groupby([\"unique_carrier\", \"day\"])[\"dep_delay\"]\n",
    " .mean()\n",
    " .unstack()\n",
    " .sort_values(by=0)\n",
    " .pipe((sns.heatmap, 'data'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about some other exploration? Pandas alows for some nifty ways of slicing up data to flexibly apply basic operations.\n",
    "\n",
    "What if we want to \"center\" the carrier's delay time at an airport by the mean airport delay? This is a case where we assigning variables might be useful. We'll limit our analysis to the top carrriers / airports, and save some variables for further interactive use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_airport_codes = flights[\"origin\"].value_counts().to_frame().head(5).index\n",
    "top_airport_cities = flights[\"origin_city_name\"].value_counts().head(5).index\n",
    "top_airport_cities\n",
    "top_airport_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_airport_delay = (flights\n",
    " .query(\"unique_carrier in @top_carriers\")\n",
    " .query(\"origin in @top_airport_codes\")\n",
    " .groupby(\"origin\")[\"dep_delay\"] \n",
    " .mean()\n",
    " .dropna()\n",
    " .to_frame()\n",
    ")\n",
    "\n",
    "airport_delay = (flights\n",
    ".query(\"unique_carrier in @top_carriers\")\n",
    ".query(\"origin in @top_airport_codes\")\n",
    " .set_index(\"fl_date\")\n",
    " .groupby([pd.TimeGrouper(\"H\"), \"origin\"])[\"dep_delay\"] \n",
    " .mean()\n",
    " .to_frame()\n",
    ")\n",
    "\n",
    "carrier_delay = (flights\n",
    ".query(\"unique_carrier in @top_carriers\")\n",
    ".query(\"origin in @top_airport_codes\")\n",
    " .set_index(\"fl_date\")\n",
    " .groupby([pd.TimeGrouper(\"H\"), \"origin\", \"unique_carrier\"])[\"dep_delay\"] \n",
    " .mean()\n",
    " .to_frame()\n",
    ")\n",
    "\n",
    "airport_delay.head()\n",
    "carrier_delay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_airport_delay\n",
    "airport_delay.unstack().head()\n",
    "carrier_delay.unstack(1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas handles alignment along axes, so we can do an operation along an axis with another dataframe with similar index labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(carrier_delay\n",
    " .unstack(1)\n",
    " .div(grand_airport_delay.unstack())\n",
    " .head()\n",
    " \n",
    ")\n",
    "(carrier_delay\n",
    " .unstack(1)\n",
    " .div(airport_delay.unstack())\n",
    " .head()\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting that together, we can then get ratios of flight delays to the overall airport delay (grand mean or daily delays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(carrier_delay\n",
    " .unstack(1)\n",
    " .div(airport_delay.unstack())\n",
    " .stack()\n",
    " .reset_index()\n",
    " .assign(day=lambda x: x[\"fl_date\"].dt.dayofweek)\n",
    " .set_index(\"fl_date\")\n",
    " .groupby([\"unique_carrier\", \"day\"])\n",
    " .mean()\n",
    " .dropna()\n",
    " .unstack()\n",
    " [\"dep_delay\"]\n",
    " .pipe((sns.heatmap, 'data'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(carrier_delay\n",
    " .unstack(1)\n",
    " .div(grand_airport_delay.unstack())\n",
    " .stack()\n",
    " .reset_index()\n",
    " .assign(day=lambda x: x[\"fl_date\"].dt.dayofweek)\n",
    " .set_index(\"fl_date\")\n",
    " .groupby([\"unique_carrier\", \"day\"])\n",
    " .mean()\n",
    " .dropna()\n",
    " .unstack()\n",
    " [\"dep_delay\"]\n",
    " .pipe((sns.heatmap, 'data'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(carrier_delay\n",
    " .unstack(1)\n",
    " .subtract(airport_delay.unstack())\n",
    " .stack()\n",
    " .reset_index()\n",
    " .assign(day=lambda x: x[\"fl_date\"].dt.dayofweek)\n",
    " .groupby([\"unique_carrier\", \"day\"])\n",
    " .mean()\n",
    " .dropna()\n",
    " .unstack()\n",
    " [\"dep_delay\"]\n",
    " .pipe((sns.heatmap, 'data'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we have some working flight data, let's poke at getting some tweets.\n",
    "\n",
    "## Tweets\n",
    "\n",
    "I've recently refactored the python gnip search api to be a bit more flexible, including making each search return a lazy stream. There are also some tools for programatically generated\n",
    "\n",
    "the 'city name' column and the airport abbreviation are likely sources of help for finding tweets related to flights / airport data. We'll use those and define a small function to help quickly generate our rules, which are somewhat simplistic but should serve as a reasonable start.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(codes, cities):\n",
    "    base_rule = \"\"\"\n",
    "    ({code} OR \"{city} airport\") (flying OR flight OR plane OR jet)\n",
    "    -(football OR\n",
    "    basketball OR\n",
    "    baseball OR\n",
    "    party)\n",
    "    -is:retweet\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    for code, city in zip(list(codes), list(cities)):\n",
    "        _rule = base_rule.format(code=code, city=city.lower())\n",
    "        rule = gen_rule_payload(_rule,\n",
    "                                from_date=\"2017-01-01\",\n",
    "                                to_date=\"2017-07-31\",\n",
    "                                max_results=500)\n",
    "        rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "gnip_rules = generate_rules(top_airport_codes, top_airport_cities)\n",
    "gnip_rules[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the gnip api has some functions to handle our connection information. Please ensure that the environment variable `GNIP_PW` is set with your password. If it isn't already set, you can set it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.environ[\"GNIP_PW\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = \"agonzales@twitter.com\"\n",
    "search_api = \"fullarchive\"\n",
    "account_name = \"shendrickson\"\n",
    "endpoint_label = \"ogformat.json\"\n",
    "og_search_endpoint = gen_endpoint(search_api,\n",
    "                                  account_name,\n",
    "                                  endpoint_label,\n",
    "                                  count_endpoint=False)\n",
    "og_args = {\"username\": username,\n",
    "           \"password\": os.environ[\"GNIP_PW\"],\n",
    "           \"url\": og_search_endpoint}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `get_tweets` function, we wrap some of the functionality of our result stream to collect specific data from tweets into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets(result_stream, label):\n",
    "    fields = [\"id\", \"created_at_datetime\",\n",
    "              \"all_text\", \"hashtags\", \"user_id\",\n",
    "              \"user_mentions\", \"screen_name\"]\n",
    "    \n",
    "    tweet_extracts = []\n",
    "    for tweet in result_stream.start_stream():\n",
    "        attrs = [tweet.__getattribute__(field) for field in fields]\n",
    "        tweet_extracts.append(attrs)\n",
    "        \n",
    "    result_stream.end_stream()\n",
    "    df = pd.DataFrame(tweet_extracts, columns=fields).assign(airport=label)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this with a single rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs = ResultStream(**og_args, rule_payload=gnip_rules[0], max_results=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = get_tweets(result_stream=rs, label=top_airport_codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's collect tweets for each airport. It might be a hair overkill, but I'll  wrap the process up in a function, so we have a similar high point for grabbing our inital data. It will take a minute to grab this data, and for the time being, i'm not going to save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_tweet_data(gnip_rules, results_per_rule=25000):\n",
    "    streams = [ResultStream(**og_args,\n",
    "                            rule_payload=rp,\n",
    "                            max_results=results_per_rule)\n",
    "               for rp in gnip_rules]\n",
    "\n",
    "    tweets = [get_tweets(rs, airport)\n",
    "              for rs, airport\n",
    "              in zip(streams, top_airport_codes)]\n",
    "    \n",
    "    return pd.concat(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pull_tweet_data(gnip_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our new data, let's do some quick exploration and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .set_index(\"created_at_datetime\")\n",
    " .groupby([pd.TimeGrouper(\"D\")])\n",
    " .size()\n",
    " .sort_values()\n",
    " .tail()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "(tweets\n",
    " .set_index(\"created_at_datetime\")\n",
    " .groupby([pd.TimeGrouper(\"D\")])\n",
    " .size()\n",
    " .plot()\n",
    ")\n",
    " \n",
    "ax.annotate(\"united senselessly\\nbeating a passenger\",\n",
    "            xytext=(\"2017-02-01\", 1200),\n",
    "            xy=(\"2017-04-04\", 900),\n",
    "            arrowprops=dict(facecolor=\"black\", shrink=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tweets per day by airport rule is a bit odd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .drop([\"id\", \"all_text\"], axis=1)\n",
    " .set_index(\"created_at_datetime\")\n",
    " .groupby([pd.TimeGrouper(\"H\"), \"airport\"])\n",
    " [\"user_id\"]\n",
    " .count()\n",
    " .unstack()\n",
    " .fillna(0)\n",
    " .rolling(24).sum()\n",
    " .plot()\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets look at what is going on with LAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"airport\"].value_counts()\n",
    "tweets.groupby(\"airport\")[\"created_at_datetime\"].min().sort_values().tail(1)\n",
    "min_lax_time = tweets.groupby(\"airport\")[\"created_at_datetime\"].min().sort_values().tail(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Far, far more people tweeting from  LAX than from other airports or the number of extra tweets were dominated by the spikes in the data. Given it's size, this makes some sense, but i would question my rules a bit. Let's even out these samples a hair, by selecting tweets only from when LAX exisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    ".drop([\"id\", \"all_text\"], axis=1)\n",
    ".query(\"created_at_datetime >= @min_lax_time\")\n",
    " .set_index(\"created_at_datetime\")\n",
    " .groupby([pd.TimeGrouper(\"D\"), \"airport\"])\n",
    " [\"user_id\"]\n",
    " .count()\n",
    " .unstack()\n",
    " .fillna(0)\n",
    " #.rolling(7).mbean()\n",
    " .plot()\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, let's do some more things with our tweets, like parse out the mentions from the dict structure to something more useful.\n",
    "\n",
    "\n",
    "We'll be making a function that takes a dataframe and returns one, so we can use it in the `.pipe` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_mentions(df):\n",
    "    extract_mentions = lambda x: [d[\"name\"] for d in x]\n",
    "    mentions = (pd.DataFrame([x for x in df[\"user_mentions\"]\n",
    "                              .apply(extract_mentions)])\n",
    "                .loc[:, [0, 1]]\n",
    "                .rename(columns={0: \"mention_1\", 1: \"mention_2\"})\n",
    "               )\n",
    "    \n",
    "    return (pd.merge(df,\n",
    "                      mentions,\n",
    "                      left_index=True,\n",
    "                      right_index=True)\n",
    "             .drop(\"user_mentions\", axis=1)\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_name_code_dict = {\n",
    "    \"Southwest Airlines\": \"WN\",\n",
    "    \"Delta\": \"DL\",\n",
    "    \"American Airlines\": \"AA\",\n",
    "    \"United Airlines\": \"UA\",\n",
    "    \"Sky\": \"Sk\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, what about labeling a row with strictly American Airlines mentions? We could do this a few ways... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .pipe(parse_mentions)\n",
    " .assign(AA=lambda df: (df[\"mention_1\"] == \"American Airlines\") |\n",
    "         (df[\"mention_2\"] == \"American Airlines\"))\n",
    " .query(\"AA == True\")\n",
    " .head()\n",
    " \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .pipe(parse_mentions)\n",
    " .query(\"mention_1 == 'American Airlines' or mention_2 == 'American Airlines'\")\n",
    " .shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .pipe(parse_mentions)\n",
    " .query(\"mention_1 == 'American Airlines' or mention_2 == 'American Airlines'\")\n",
    " .query(\"created_at_datetime >= @min_lax_time\")\n",
    " .set_index(\"created_at_datetime\")\n",
    " .groupby([pd.TimeGrouper(\"D\"), \"airport\"])\n",
    " [\"user_id\"]\n",
    " .count()\n",
    " .unstack()\n",
    " .fillna(0)\n",
    " .rolling(7).mean()\n",
    " .plot()\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on a bit, what about a simple sentiment model? We'll grab a word database that simply matches words to a value and use it as a simple baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def get_affin_dict():\n",
    "    url = \"https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-111.txt\"\n",
    "    affin_words = (pd\n",
    "                   .read_table(url,\n",
    "                               sep='\\t',\n",
    "                               header=None)\n",
    "                   .rename(columns={0: \"word\", 1: \"score\"})\n",
    "                   .to_dict(orient=\"list\")\n",
    "                  )\n",
    "    affin_words = {k: v for k, v in\n",
    "                   zip(affin_words[\"word\"],\n",
    "                       affin_words[\"score\"])}\n",
    "    return affin_words\n",
    "\n",
    "\n",
    "\n",
    "tknizer = TweetTokenizer()\n",
    "\n",
    "def score_sentiment(words):\n",
    "    words = set(words) \n",
    "    union = words & affin_words.keys()\n",
    "    return sum([affin_words[w] for w in union])\n",
    "    \n",
    "def score_tweet(tweet_text):\n",
    "    return score_sentiment(tknizer.tokenize(tweet_text))\n",
    "\n",
    "affin_words = get_affin_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .assign(sentiment=lambda df: df[\"all_text\"].apply(score_tweet))\n",
    " [\"sentiment\"]\n",
    " .plot.hist(bins=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .assign(sentiment=lambda df: df[\"all_text\"].apply(score_tweet))\n",
    " .pipe(lambda df: pd.concat([df.query(\"sentiment <= -5\").head(),\n",
    "                            df.query(\"sentiment >= 5\").head()]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems semi-reasonable to me!\n",
    "\n",
    "Let's look at a timeseries of sentiment overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .assign(sentiment=lambda df: df[\"all_text\"].apply(score_tweet))\n",
    " .set_index(\"created_at_datetime\")\n",
    " .groupby([pd.TimeGrouper(\"D\")])\n",
    " [\"sentiment\"]\n",
    " .mean()\n",
    " .rolling(2).mean()\n",
    " .plot()\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have our reasonable sentiment and mentions data, let's assign it to a fresh dataframe and continue looking. \n",
    "\n",
    "Note that a full data pull step at this point might look like\n",
    "\n",
    "```.python\n",
    "tweets = (pull_tweet_data(gnip_rules)\n",
    "          .pipe(parse_mentions)\n",
    "          .assign(sentiment=lambda df: df[\"all_text\"].apply(score_tweet))\n",
    "         )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = (tweets\n",
    "          .pipe(parse_mentions)\n",
    "          .assign(sentiment=lambda df: df[\"all_text\"].apply(score_tweet))\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's do some basic exploration of our tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .groupby([\"airport\"])\n",
    " [\"sentiment\"]\n",
    " .mean()\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .assign(day=lambda x: x.created_at_datetime.dt.dayofweek)\n",
    " .assign(hour=lambda x: x.created_at_datetime.dt.hour)\n",
    " .groupby([\"day\", \"hour\"])\n",
    " [\"sentiment\"]\n",
    " .mean()\n",
    " .unstack()\n",
    " .pipe((sns.heatmap, 'data') )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .assign(hour=lambda x: x.created_at_datetime.dt.hour)\n",
    " .assign(day=lambda x: x.created_at_datetime.dt.dayofweek)\n",
    " .groupby([\"airport\", \"day\"])\n",
    " [\"sentiment\"]\n",
    " .mean()\n",
    " .unstack()\n",
    " .sort_values(by=0)\n",
    " .pipe((sns.heatmap, 'data') )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .assign(hour=lambda x: x.created_at_datetime.dt.hour)\n",
    " .groupby([\"airport\", \"hour\"])\n",
    " [\"sentiment\"]\n",
    " .mean()\n",
    " .unstack()\n",
    " .sort_values(by=0)\n",
    " .pipe((sns.heatmap, 'data'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tweets\n",
    " .assign(day=lambda x: x.created_at_datetime.dt.dayofweek)\n",
    " .query(\"airport == 'ATL' and day == 2\")\n",
    " .sample(10)\n",
    " .all_text\n",
    "       )\n",
    "\n",
    "(tweets\n",
    " .assign(day=lambda x: x.created_at_datetime.dt.dayofweek)\n",
    " .query(\"airport == 'ATL' and day == 5\")\n",
    " .sample(10)\n",
    " .all_text\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_sent_airport = (tweets\n",
    " .set_index(\"created_at_datetime\")\n",
    " .groupby([pd.TimeGrouper(\"D\"), \"airport\"])[\"sentiment\"]\n",
    " .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delay_sent = (pd.concat([airport_delay, tweet_sent_airport],\n",
    "                        axis=1,\n",
    "                        names=(\"day\", \"airport\"))\n",
    "              .sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for code in top_airport_codes:\n",
    "    (delay_sent\n",
    "     .loc[pd.IndexSlice[:, code], :]\n",
    "     .plot(subplots=True, title=\"Sentiment and delay time at {}\".format(code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_sent.loc[pd.IndexSlice[:, \"ATL\"], :].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_sent.groupby(level=1).corr().T.loc[\"sentiment\"].unstack()[\"dep_delay\"].sort_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
