{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Time Series Modeling, Pt. 3: Seasonal-Trend Decomposition\n",
    "\n",
    "2017-{07..08}, Josh Montague\n",
    "\n",
    "\n",
    "This is Part 3 of the [\"Time Series Modeling in Python\" series](https://github.com/DrSkippy/Data-Science-45min-Intros/tree/master/time-series). \n",
    "\n",
    "If you'd like to hop back to earlier sections, recall:\n",
    "- In [Part 1](https://github.com/DrSkippy/Data-Science-45min-Intros/blob/master/time-series/01%20-%20Time%20series%20data%20in%20pandas.ipynb), we looked at data structures within the `pandas` library that make working with time series particularly convenient.\n",
    "- In [Part 2](https://github.com/DrSkippy/Data-Science-45min-Intros/blob/master/time-series/02%20-%20Simple%20time%20series%20models.ipynb), we looked at the some of the simplest methods for modeling a time series, making forecasts, and evaluating the accuracy of our models. \n",
    "\n",
    "In this section, we'll look at a common pattern for modeling time series data that takes greater advantage of prior observations in the series. Specifically, we're going to look at breaking down an observed time series into components that represent underlying trends and seasonality. Additionally, we'll write and use an \"STL decomposition\" because it's a nice interpretable model that doesn't exist in `statsmodels` and I think it should. \n",
    "\n",
    "\n",
    "The general outline we'll follow is:\n",
    "- start with a motivating example (\"start with the answer\")\n",
    "- build our own version to see how it works\n",
    "- look at any alternative implementation\n",
    "\n",
    "Consistent with earlier parts of the series, [the OText content on forecasting](https://www.otexts.org/fpp) is a great reference, and we'll follow some of the conventions there, particularly Chapter 6.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rc('figure', figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Decomposing a time series\n",
    "\n",
    "\n",
    "In the previous parts of this series, our modeling (and forecasting) efforts often ignored most of the data in our time series. Our predictions were either simply based on the last-observed point, or a single point some relatively small distance in the past. \n",
    "\n",
    "Now, we'll consider larger-scale patterns in the time series data. Often these patterns are described in terms of a decomposition into \"components.\" The set of components we'll refer to in this session is:\n",
    "\n",
    "- trend: long-term (low-frequency, large-period), low-variance changes in the data \n",
    "- seasonal: fixed-period (or frequency) fluctuations in the data (day, week, month, etc)\n",
    "- residual (or error): any difference remaining after the previous components are removed from observed data\n",
    "\n",
    "We can write out the combined model as:\n",
    "\n",
    "`y = S + T + E,` if the model is additive\n",
    "\n",
    "or\n",
    "\n",
    "`y = S * T * E,` if the model is multiplicative.\n",
    "\n",
    "\n",
    "The additive model makes sense when the magnitude of the seasonal component doesn't vary with the absolute level of the series, and the multiplicative model makes sense when that variation is present. This is often an emperical choice, and the additive model is often the default setting.\n",
    "\n",
    "The specific implementation of both the \"trend\" and \"seasonal\" components are variable, and we'll look at a couple of examples below.\n",
    "\n",
    "\n",
    "# Get some data\n",
    "\n",
    "Like many libraries, statsmodels has some datasets built-in. This dataset represents daily observations of CO$_2$ in Hawaii over the span of forty years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# http://www.statsmodels.org/stable/datasets/index.html\n",
    "dataset = sm.datasets.co2.load_pandas()\n",
    "\n",
    "# special sm object (\"bunch\"-like cf sklearn datasets)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# has a pd.dataframe in the .data attr\n",
    "co2_ts = dataset.data\n",
    "\n",
    "co2_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "co2_ts.plot(title='Mauna Loa Weekly Atmospheric CO2 Data')\n",
    "\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('CO2 concentration (ppmv)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Right off the bat, we can identify some qualities of this data\n",
    "\n",
    "- ✅  general \"trend\" (up and right)\n",
    "- ✅  seasonality (wiggles that look yearly)\n",
    "- ✅  missing data (some small gaps)\n",
    "\n",
    "Since many timeseries models don't play nicely with missing data or NaNs, let's fill in the missing data with linear interpolations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# how much missing data?\n",
    "len(co2_ts.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# often, ts methods/libs don't play nicely with missing data/NaNs, so\n",
    "#  fill missing values (recall pd timeseries methods from Part 1!)\n",
    "co2_ts = (co2_ts\n",
    "          .resample('D')\n",
    "          .mean()\n",
    "          .interpolate('linear')\n",
    "         )          \n",
    "          \n",
    "co2_ts.head(10)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "co2_ts.plot()\n",
    "\n",
    "# if you want a close-up\n",
    "#co2_ts.head(1000).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Seasonal decomposition with `statsmodels` \n",
    "\n",
    "To motivate the work, I'll start with the answer that statsmodels provides out-of-the-box. Then we'll look under the hood to build some of it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sm has recently been updated to accept pd.dataframes in more places (previously all numpy arrays)\n",
    "seas_decomp = seasonal_decompose(co2_ts, model='additive', freq=365)\n",
    "\n",
    "seas_decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use `stl.<TAB>` above to see the attributes of this object. It's simple: a few arrays and a `.plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the DecomposeResult doesn't expose very much for you, just arrays and a mpl.figure convenience method\n",
    "fig = seas_decomp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This figure highlights the goal: take the observed data (top), identify the overall trend (a low-variance version of the observations, second chart), and identify the periodic component (called \"seasonal,\" regardless of its frequency). Then, whatever is left after subtracting these from the observed data is the residual (bottom chart). Ideally, we're left with small-amplitude, Gaussian-ish residuals. In this example, it looks like relatively random, zero-centered jitters, at a level that is a fraction of a percent of the observations. That's good!\n",
    "\n",
    "\n",
    "\n",
    "**Note**: inspecting the data for most significant periodicity is important! By default, statsmodels looks inside the pandas frame, and [maps the frequency of the index to a filter frequency](http://www.statsmodels.org/stable/_modules/statsmodels/tsa/tsatools.html#freq_to_period). Sounds reasonable, but here, the daily data leads to a filter function that is \"7\" e.g. a weekly smoothing. However, our data doesn't have weekly periodicity so the trend ends up overfit (see charts just below). We have to look at our data observe that the major periodicity is annual, so (above) we set the frequency to 365 (number in indices given the units of the observed data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# default settings -> read the pd.datetimeindex freq (1 day), assumes weekly cycles\n",
    "seas_decomp = seasonal_decompose(co2_ts)\n",
    "\n",
    "# overfit!\n",
    "seas_decomp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build it ourselves\n",
    "\n",
    "Recall, we have basically two goals:\n",
    "\n",
    "- calculate and subtract trend (smooth)\n",
    "- calculate and subtract seasonality\n",
    "\n",
    "\n",
    "### Trend component\n",
    "\n",
    "The `seasonal_decompose` method isn't technically an \"STL\" (Seasonal Trend with Lowess) decomposition because it doesn't use the Loess method (more on this soon), but it does decompose the signal into a seasonal and trend component. The `statsmodels` function uses a convolution between the observed data and a filter (roughly, a square wave) defined by the specified frequency of the seasonal component. \n",
    "\n",
    "The method will introspect a dataframe for the frequency of it's datetimeindex, but you may have to manually override it. If you have a month of daily sales data, the largest seasonal component is probably weekly. In our example, we have years of daily weather data and the biggest seasonal component is the annual one (365). You could imagine using a Fourier Transform to identify the largest seasonal component automatically, but this is left as an exercise for the reader. \n",
    "\n",
    "So what is a [convolution](https://en.wikipedia.org/wiki/Convolution)? Generally it's an operation that takes two functions, and produces another. Roughly, it's the integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated. I always remember it best as \"smearing one curve across another while dragging it from left to right.\" I find this figure (from the wiki page) helpful.\n",
    "\n",
    "<img src='misc/conv.png' width=500>\n",
    "\n",
    "We don't have to go so far as writing our own convolution. This one from statsmodels is built around the relevant tools from scipy. Below, I'll follow, briefly, what the `seasonal_decompose()` method is doing under the hood. Don't get too hung up on the concept of the convolution; it's just one way to smooth the observed data into a trend that happens to account for variation over a particular window size. We'll look at another example shortly.\n",
    "\n",
    "We'll calculate a few things, then plot them all together for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.filters.filtertools import convolution_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the filter size (units = # of data points)\n",
    "freq = 365\n",
    "\n",
    "# an array that is a square wave from 0 to freq\n",
    "filt = np.repeat(1./freq, freq)\n",
    "#filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of data points to include in inspection charts\n",
    "nobs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert our dataframe of observations to 1-d np.array\n",
    "observed = np.asanyarray(co2_ts).squeeze()\n",
    "\n",
    "observed[:nobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# calculate the \"trend\" array by dragging filter across data\n",
    "trend = convolution_filter(observed, filt)\n",
    "\n",
    "#trend[:nobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# subtract the trend array (\"standardize\" values)\n",
    "detrended = observed - trend\n",
    "\n",
    "# these values should be closer to 0\n",
    "detrended[:nobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(31, sharex=True)\n",
    "\n",
    "# filter\n",
    "plt.subplot(311)\n",
    "# note: i'm artificially adding a bunch of 0s to the filter here so we can see it on the same x scale\n",
    "plt.plot(np.append(filt, np.array([0]*(nobs - len(filt)))), '.-', label='filter')\n",
    "plt.title(\"first {} observed data points\".format(nobs))\n",
    "plt.legend()\n",
    "\n",
    "# trend\n",
    "plt.subplot(312)\n",
    "plt.plot(observed[:nobs], '.-', label='observed')\n",
    "plt.plot(trend[:nobs], '.-', label='trend')\n",
    "plt.legend()\n",
    "\n",
    "# standardized / de-trended\n",
    "plt.subplot(313)\n",
    "plt.plot(detrended[:nobs], '.-', label='detrended')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Seasonal component\n",
    "\n",
    "Now we use the detrended series to find the seasonal component. In statsmodels, they do this with an interesting empirical method. They calculate the mean value for reach of the `freq` points at indexes with the specified periodicity. That is, the mean across the values at index 0, 365, 729, etc., the mean across the values at index 1, 366, 730, etc., and so on, until we've covered each of the `freq` e.g. 365 possible means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# special handling of mean wrt NaN values\n",
    "from pandas.core.nanops import nanmean as pd_nanmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate mean value in each point of the seasonal cycle (defined by freq)\n",
    "#  e.g. each day in the annual cycle\n",
    "period_averages = np.array([pd_nanmean(detrended[i::freq]) for i in range(freq)])\n",
    "\n",
    "# center average cycle \n",
    "period_averages -= np.mean(period_averages)\n",
    "\n",
    "len(period_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(period_averages, '.-', label='seasonal cycle')\n",
    "\n",
    "plt.xlabel('point in cycle')\n",
    "plt.ylabel('mean value')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tile out the seasonal curve - this becomes the seasonal part of the model \n",
    "seasonal = np.tile(period_averages, len(observed) // freq +1)[:len(observed)]\n",
    "\n",
    "#len(seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(seasonal[:nobs], '.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the residual is what's left over after the seasonal component \n",
    "#  is subtracted from the detrended data\n",
    "resid = detrended - seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have all the pieces we need to construct the `seasonal_decompose` output: trend and seasonal components, and the residual measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# decomposition details (first `nobs` points)\n",
    "fig, axes = plt.subplots(41, sharex=True)\n",
    "\n",
    "# trend\n",
    "plt.subplot(411)\n",
    "plt.plot(observed[:nobs], '.-', label='observed')\n",
    "plt.plot(trend[:nobs], '-', label='trend')\n",
    "plt.title('decomposition on first {} points'.format(nobs))\n",
    "plt.legend()\n",
    "\n",
    "# detrended\n",
    "plt.subplot(412)\n",
    "plt.plot(detrended[:nobs], '-', label='detrended')\n",
    "plt.legend()\n",
    "\n",
    "# seasonal\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal[:nobs], '-', label='seasonal')\n",
    "plt.legend()\n",
    "\n",
    "# residual\n",
    "plt.subplot(414)\n",
    "plt.plot(resid[:nobs], '-', label='residual')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# all data points \n",
    "fig, axes = plt.subplots(41, sharex=True)\n",
    "\n",
    "# trend\n",
    "plt.subplot(411)\n",
    "plt.plot(observed, '-', label='observations')\n",
    "plt.plot(trend, '-', label='trend')\n",
    "plt.title('decomposition on all points')\n",
    "plt.legend()\n",
    "\n",
    "# detrended\n",
    "plt.subplot(412)\n",
    "plt.plot(detrended, '-', label='detrended')\n",
    "plt.legend()\n",
    "\n",
    "# seasonal\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, '-', label='seasonal')\n",
    "plt.legend()\n",
    "\n",
    "# residuals\n",
    "plt.subplot(414)\n",
    "plt.plot(resid, '-', label='residual')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# look at the distribution of residuals ( hist() doesn't like NaNs )\n",
    "plt.hist([ x for x in resid if not np.isnan(x) ], bins=100);\n",
    "\n",
    "plt.xlabel('residual')\n",
    "plt.ylabel('count')\n",
    "plt.title('distribution of residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If the residual distribution was really skewed, we might consider going back to our assumptions of seasonal frequency, and possibly modify our filter (in the convolutional example). At the very least, it would set up our expectations for how any predictions would over- or under-estimate real values. \n",
    "\n",
    "\n",
    "The `seasonal_decompose` method in statsmodels is perfectly fine, but I've long wanted statsmodels to have a proper STL decomposition. There are many libraries full of advanced methods for time series forecasting in Python (and we'll likely see some later in this series!), but not as many that are simple. I value simplicity and interpretability, both of which I think STL embodies. Looking at the functionality of R's `forecast` library can be a little depressing for Pythoneers in this regard (Hyndman is prolific). \n",
    "\n",
    "So, if it doesn't exist? Let's just make it!\n",
    "\n",
    "\n",
    "## Lowess & STL decomposition \n",
    "\n",
    "The STL (Seasonal-Trend with Lowess) decomposition was published in 1990 by Cleveland, as an application of the Lowess (locally weighted scatterplot smoothing) method published by the same author in 1979. statsmodels doesn't have an out-of-the-box STL decomp method, but it does have [an implementation of the Lowess smoother](http://www.statsmodels.org/dev/generated/statsmodels.nonparametric.smoothers_lowess.lowess.html). \n",
    "\n",
    "Lowess is generally a super handy way of highlighting trends in noisy data. It's an underutilized tool (I think) that also shows up in e.g. [ggplot](http://ggplot2.tidyverse.org/reference/geom_smooth.html#examples), and now [seaborn](http://seaborn.pydata.org/generated/seaborn.lmplot.html). Before we use it in a decomposition, let's make sure we understand what it's doing by itself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lowess = sm.nonparametric.lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make some noisy data\n",
    "nobs = 200\n",
    "\n",
    "x = np.linspace(0,nobs-1,nobs)\n",
    "\n",
    "# sine-ish\n",
    "#y = 0.02*x + np.sin(0.25*x) + np.random.normal(scale=0.25, size=len(x))\n",
    "\n",
    "# linear-ish\n",
    "y = 0.02*x + np.random.normal(scale=0.75, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Unfortunately, statsmodels has a different API philosophy than, say, sklearn. So the `(exog, endog)`  function signature can take some getting used to if you're used to seeing the `(X, y)` signature pattern. But, ultimately, they just take some time to translate in your mind.\n",
    "\n",
    "There are a couple of parameters in a Lowess model, but the only one that affects the fitting is `frac` which controls how much adjacent data to use in calculating any one point in the smoothed curve. frac is the fraction (of the entire array of data) adjacent to the relevant point that is used. \n",
    "\n",
    "The regression is locally weighted - meaning the importance of some y values is modified - by a function that has a form roughly between a short-tailed Gaussian and a rounded square wave.\n",
    "\n",
    "Let's compare output, varying the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# default frac=0.66\n",
    "z = lowess(y, x, return_sorted=False)\n",
    "\n",
    "# compare to a couple extreme values of frac\n",
    "frac1 = 0.1\n",
    "frac2 = 0.9\n",
    "\n",
    "w = lowess(y, x, frac=frac1, return_sorted=False)\n",
    "v = lowess(y, x, frac=frac2, return_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y, '.', label='obs')\n",
    "\n",
    "plt.plot(z, '--', label='frac=0.6 (default)')\n",
    "plt.plot(w, '--', label='frac={}'.format(frac1))\n",
    "plt.plot(v, '--', label='frac={}'.format(frac2))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note: the importance of the `frac` kwarg changes with the number of data points. Small N (total observations) means a much higher variance model than large N for the same `frac` value.\n",
    "\n",
    "### Drop in for convolution\n",
    "\n",
    "This time, I'll skip the drawn out version we used previously and drop the Lowess smoothing in as the trend calculation in a decomposition. I'm also going to import some utilities directly from statsmodels. We'll talk through them, and if you're curious for more, you can poke around in the statsmodels source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stl_decompose(df, freq=365, lo_frac=0.6, lo_delta=0.0):\n",
    "    \"\"\"\n",
    "    An STL decomposition of time series data.\n",
    "    \n",
    "    This implementation is modeled after the statsmodels seasonal_decompose method\n",
    "    but substitutes Lowess for convolution in its trend estimation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.Dataframe\n",
    "        Time series of counts with a DatetimeIndex.\n",
    "    freq: int, optional\n",
    "        Most significant periodicity in the time series.\n",
    "    lo_frac: float, optional\n",
    "        Fraction of data to use in Lowess smoothing\n",
    "    lo_delta: float, optional\n",
    "        Fractional distance within which to use linear-interpolation \n",
    "        instead of weighted regression. Significantly affects \n",
    "        computation time!\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : obj\n",
    "        A object with seasonal, trend, and resid attributes.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is an additive model, Y[t] = T[t] + S[t] + e[t]        \n",
    "    \"\"\"\n",
    "    # use some existing pieces of statsmodels\n",
    "    from statsmodels.tsa.seasonal import DecomposeResult\n",
    "    from statsmodels.tsa.filters._utils import _maybe_get_pandas_wrapper_freq\n",
    "    import statsmodels.api as sm\n",
    "    from pandas.core.nanops import nanmean as pd_nanmean\n",
    "    \n",
    "    lowess = sm.nonparametric.lowess\n",
    "    _pandas_wrapper, _ = _maybe_get_pandas_wrapper_freq(df)\n",
    "    \n",
    "    # get plain np array\n",
    "    observed = np.asanyarray(df).squeeze()\n",
    "    \n",
    "    # calc trend, remove from observation\n",
    "    trend = lowess(observed, [x for x in range(len(observed))], \n",
    "                   frac=lo_frac, \n",
    "                   delta=lo_delta*len(observed),\n",
    "                   return_sorted=False)\n",
    "    detrended = observed - trend\n",
    "    \n",
    "    # calc one-period seasonality, remove tiled array from detrended\n",
    "    period_averages = np.array([pd_nanmean(detrended[i::freq]) for i in range(freq)])\n",
    "    # 0-center the period avgs\n",
    "    period_averages -= np.mean(period_averages)\n",
    "    seasonal = np.tile(period_averages, len(observed) // freq + 1)[:len(observed)]    \n",
    "    resid = detrended - seasonal\n",
    "    \n",
    "    # convert the arrays back to appropriate dataframes, stuff them back into \n",
    "    #  the statsmodel object\n",
    "    results = list(map(_pandas_wrapper, [seasonal, trend, resid, observed]))    \n",
    "    dr = DecomposeResult(seasonal=results[0],\n",
    "                         trend=results[1],\n",
    "                         resid=results[2], \n",
    "                         observed=results[3],\n",
    "                         period_averages=period_averages)\n",
    "        \n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "co2_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use just like the seasonal_decompose method\n",
    "co_stl = stl_decompose(co2_ts, freq=365, lo_frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we inherit a .plot() method from DecomposeResult\n",
    "co_stl.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# or make your own custom figure with the internal df attributes\n",
    "fig, axes = plt.subplots(41)\n",
    "\n",
    "# obs + trend\n",
    "plt.subplot(411)\n",
    "plt.plot(co_stl.observed, '-', label='x')\n",
    "plt.plot(co_stl.trend, '-', label='Lowess trend')\n",
    "plt.legend()\n",
    "\n",
    "# seasonal\n",
    "plt.subplot(412)\n",
    "plt.plot(co_stl.seasonal, '-', label='seasonal')\n",
    "plt.legend()\n",
    "\n",
    "# residuals\n",
    "plt.subplot(413)\n",
    "plt.plot(co_stl.resid, '-', label='residual')\n",
    "plt.legend()\n",
    "\n",
    "# resid hist\n",
    "plt.subplot(414)\n",
    "plt.hist(co_stl.resid.values, bins=100)\n",
    "plt.xlabel('residual')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also pass through an extra kwarg that the Lowess method supports which allows us to not fit a regression for *every* point, but to linearly interpolate any point that's within some distance of the last one calculated. This lets us trade some accuracy for less runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this will take ~1-2 minutes\n",
    "%timeit -n3 -r3 stl_decompose(co2_ts, lo_frac=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# don't recalc regression w/in 1% windows \n",
    "%timeit -n3 -r3 stl_decompose(co2_ts, lo_frac=0.25, lo_delta=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Forecasting with seasonal decomposition\n",
    "\n",
    "Now that we have a decomposition, it'd be great if we could project it forward. Continuing to follow the methods as before, we can consider the components of the decomposition:\n",
    "\n",
    "`y = S + T + E`, \n",
    "\n",
    "forecast them separately and then combine them appropriately. For now, we're only going to consider additive models, and we're not going to incorporate `E` (maybe later!).\n",
    "\n",
    "We'll start by forecasting the trend component. In the previous session, we defined a set of simple one-point-ahead forecasting methods that we can reuse here. We can import them from the module included in this repo.\n",
    "\n",
    "The workflow we aim for is:\n",
    "\n",
    "- decompose an observed time series using the STL decomposition above\n",
    "- create a forecast of the observed series (of user-specified length) using the decomposition components, and a chosen forecasting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# some methods from earlier RST\n",
    "from rst_timeseries import fc_naive, fc_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forecast_stl(stl, steps=10, seasonal=False, fc_func=None, **fc_func_kwargs):\n",
    "    \"\"\"\n",
    "    Create an (additive) forecast, using the given forecasting function, \n",
    "    the specified number of steps forward, including optional seasonality, \n",
    "    given an STL decomposition of an observed time series.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stl : an extended statsmodels.tsa.seasonal.DecomposeResult\n",
    "        STL decomposition of observed time series\n",
    "    steps: int, optional\n",
    "        Number of forward steps to include in the forecast\n",
    "    seasonal: boolean, optional\n",
    "        Include seasonal component in forecast\n",
    "    fc_func: function\n",
    "        Forecasting function which takes an array of observations and returns a single\n",
    "        valued forecast for the next point\n",
    "    fc_func_kwargs: keyword arguments\n",
    "        All remaining arguments are passed to the forecasting function fc_func\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    forecast_frame : pd.DataFrame\n",
    "        A pandas dataframe with timeseries index corresponding to the specified \n",
    "        forecast duration\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is an additive model, Y[t] = T[t] + S[t] + e[t]    \n",
    "    \n",
    "    \"\"\"\n",
    "    ## container for forecast values\n",
    "    forecast_array = np.array([])\n",
    "    \n",
    "    ## forecast trend\n",
    "    # unpack precalculated trend array stl frame\n",
    "    #trend_array = np.asanyarray(stl.trend).squeeze()\n",
    "    trend_array = stl.trend\n",
    "    \n",
    "    # iteratively forecast trend (\"seasonally adjusted\") component\n",
    "    for step in range(steps):\n",
    "        # make this prediction on all available data\n",
    "        pred = fc_func(np.append(trend_array, forecast_array), **fc_func_kwargs)\n",
    "        # add this prediction to current array\n",
    "        forecast_array = np.append(forecast_array, pred)\n",
    "   \n",
    "    # forecast index starts one unit beyond observed series\n",
    "    ix_start = stl.observed.index[-1] + pd.Timedelta(1, stl.observed.index.freqstr)     \n",
    "    forecast_idx = pd.DatetimeIndex(freq=stl.observed.index.freqstr,\n",
    "                                    start=ix_start, \n",
    "                                    periods=steps)\n",
    "\n",
    "    ## (optionally) forecast seasonal & combine \n",
    "    if seasonal:\n",
    "        # track index and value of max correlation\n",
    "        s_ix = 0\n",
    "        max_correlation = -np.inf\n",
    "        \n",
    "        # loop over indexes=length of period avgs\n",
    "        detrended_array = np.asanyarray(stl.observed - stl.trend).squeeze()\n",
    "        for i,x in enumerate(stl.period_averages):\n",
    "            # work slices backward from end of detrended observations\n",
    "            if i == 0:\n",
    "                # slicing w/ [x:-0] doesn't work\n",
    "                detrended_slice = detrended_array[-len(stl.period_averages):]        \n",
    "            else:\n",
    "                detrended_slice = detrended_array[-(len(stl.period_averages) + i) : -i]\n",
    "            # calculate corr b/w period_avgs and detrend_slice\n",
    "            this_correlation = np.correlate(detrended_slice, stl.period_averages)[0]\n",
    "            if this_correlation > max_correlation:\n",
    "                # update ix and max correlation\n",
    "                max_correlation = this_correlation\n",
    "                s_ix = i\n",
    "        # roll seasonal signal to matching phase\n",
    "        rolled_period_averages = np.roll(stl.period_averages, -s_ix)\n",
    "        # tile as many time as needed to reach \"steps\", then truncate\n",
    "        tiled_averages = np.tile(rolled_period_averages, \n",
    "                                 (steps // len(stl.period_averages) + 1))[:steps]\n",
    "        # add seasonal values to previous forecast\n",
    "        forecast_array += tiled_averages                \n",
    "    \n",
    "    # combine data array with index into named dataframe\n",
    "    forecast_frame = pd.DataFrame(data=forecast_array, index=forecast_idx)\n",
    "    forecast_frame.columns = [fc_func.__name__]            \n",
    "    return forecast_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To demonstrate how this works, let's start by using the same dataset, but assume that we have only observed a fraction of it when we want to make our forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# take a fraction to forecast\n",
    "co2_ts_short = co2_ts.head(10000)\n",
    "\n",
    "plt.plot(co2_ts, '--', label='full')\n",
    "plt.plot(co2_ts_short, '-', label='short')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, we'll fit the STL model to our observed data set, and use the same daily frequency as before, plus some of the Lowess settings we looked at for efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model the abbreviated series with our STL decomposition\n",
    "stl = stl_decompose(co2_ts_short, freq=365, lo_frac=0.25, lo_delta=0.01)\n",
    "\n",
    "# note the last date of the observed frame \n",
    "# and compare to first date in forecast frame\n",
    "co2_ts_short.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note the final data point in our observed (and modeled) time series. Our forecast should pick up right after this point.\n",
    "\n",
    "We'll forecast out model out 1000 steps, which will cover a few periods of our data. We'll also use the naive \"drift\" forecast. Recall from the last session that this makes it's next-point prediction based on a linear fit of the last observed points and the point `n` steps back (default `n=3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now forecast forward `steps` values, with optional seasonality, and \n",
    "#  using one of our imported forecasting functions\n",
    "fc_df = forecast_stl(stl, steps=1000, seasonal=True, fc_func=fc_drift)\n",
    "fc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compare the forecast to our observations, ground truth, and the modeled trend\n",
    "plt.plot(co2_ts, ':', label='ground truth')\n",
    "plt.plot(co2_ts_short.tail(12000), label='obs')\n",
    "plt.plot(stl.trend, '--', label='stl.trend')\n",
    "plt.plot(fc_df, '.-', label='forecast')\n",
    "\n",
    "# zoom way in on the relevant region\n",
    "#plt.xlim('1980','1993'); plt.ylim(330,370);\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "<h1> 🙌🏼 🙌🏼 🙌🏼 </h1>\n",
    "</center>\n",
    "\n",
    "Just to be sure it works, let's try it on another time series...\n",
    "\n",
    "The small file `boulder.csv` included in `misc/` is an approximate daily count of the appearances of \"boulder\" on Twitter between April and mid-August."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bdr_counts = (pd.read_csv('misc/boulder.csv', parse_dates=[0])\n",
    "              .set_index('dt')\n",
    "             )\n",
    "bdr_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As before, we should fill NaNs, but in this demo there aren't actually any NaNs. However, this is still important because the resample method also ensures that our DatetimeIndex also has the right `freq` attribute we need in our STL method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fill NaNs (in this case, but also ensure that the df has freq='D' set\n",
    "bdr_counts = (bdr_counts\n",
    "              .resample('D')\n",
    "              .mean()\n",
    "              .interpolate('linear')\n",
    "             )     \n",
    "\n",
    "#bdr_counts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bdr_counts.plot()\n",
    "\n",
    "plt.ylabel('daily \"boulder\" counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(bdr_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bdr_short = bdr_counts.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# it seems the weekly cycle is biggest and data is daily, so freq=7\n",
    "stl = stl_decompose(bdr_short, freq=7, lo_frac=0.4, lo_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stl.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stl.resid.hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These residuals are certainly larger than the ones we had with our first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bdr_fc = forecast_stl(stl, steps=50, seasonal=True, fc_func=fc_drift)\n",
    "\n",
    "# plot \n",
    "plt.plot(bdr_counts, '--o', label='truth')\n",
    "plt.plot(bdr_short, '--o', label='observed')\n",
    "plt.plot(bdr_fc, '-o', label='forecast')\n",
    "plt.plot(stl.trend, ':', label='trend')\n",
    "\n",
    "plt.ylabel('daily \"boulder\" counts')\n",
    "plt.xlabel('date')\n",
    "plt.legend();\n",
    "\n",
    "# zoom into forecast region\n",
    "#plt.xlim('2017-05-15','2017-09'); plt.ylim(1500,3500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Not bad!\n",
    "\n",
    "That definitely isn't as beautiful and obvious as the first data set. But it's also a good example of where the seasonal component is not the overwhelmingly dominant feature. In this case, the daily (and weekly) fluctuations are of the same size (or bigger than) the modelled seasonal component. \n",
    "\n",
    "Another thing to observe with this model is the effect of the `lo_frac` term - that is, how narrow the bandwidth is used in the Lowesss trend. If you turn it down to ~0.3, you'll see the last few points trending up and right, leading the model (which, recall is a linear extrapolation of the fit Lowess trend) to diverge quite a lot from the observed data. \n",
    "\n",
    "So, sadly (but hopefully not surprisingly) there is no silver bullet! But, with some tuning and observation, this is one relatively straightforward method of forecasting a time series. \n",
    "\n",
    "\n",
    "# Wrap-up\n",
    "\n",
    "\n",
    "In summary, STL decompositions are pretty handy, and relatively efficient. \n",
    "\n",
    "Pro\n",
    "- interpretable \n",
    "- flexible and robust\n",
    "- arbitrary timescales and seasonality\n",
    "- takes advantage more of the observations\n",
    "\n",
    "Con\n",
    "- doesn't account for transient events\n",
    "- can be inefficient\n",
    "- still requires an expert to identify most important frequency, and Lowess bandwidth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
