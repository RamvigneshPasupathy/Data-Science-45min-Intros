{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing clustering algorithms\n",
    "\n",
    "2017-09-08, Josh Montague\n",
    "\n",
    "\n",
    "## What\n",
    "\n",
    "We often have some curated data of interest for which we'd like a low-dimensional representation. In the case of Twitter data, the corpus in question is usually based on filtering of the text content of the Tweet itself or the description of the user (profile bio). Since [Twitter is made of people](https://en.wikipedia.org/wiki/Soylent_Green), we're often in a position where we want to discover and describe a small (low-dimensional) number of \"groups\" or \"communities\" of Tweets or users within a corpus. \n",
    "\n",
    "Unsupervised learning (clustering) is the tool commonly used for this. There are 10 clustering algorithms built in to the sklearn API as of the time of this writing, and others that exist outside of that one specific library. The take-away from this session is a proposal to move from \n",
    "\n",
    "**\"KMEANS ALL THE THINGS\"** \n",
    "\n",
    "to \n",
    "\n",
    "**\"HDBSCAN ALL THE THINGS (while also taking the time to think a bit more about your algorithm assumptions)\"**. \n",
    "\n",
    "It really just rolls right off the tongue.\n",
    "\n",
    "\n",
    "## Why\n",
    "\n",
    "Most of my clustering experience has been with the KMeans algorithm, and so I can't speak from a position of much experience on all the rest. However, I do know that KMeans is often used in clustering tasks - often when it probably shouldn't be - because it is incredibly fast. I've always been uncomfortable with applying this hammer to all tasks, so the goal of this session is to highlight some alternatives and where they differ.\n",
    "\n",
    "Other people who care more deeply about the comparison of these algorithms have written about them, and so here I'll link to (and copy from) some of those references.\n",
    "\n",
    "\n",
    "## How\n",
    "\n",
    "First, we'll look at a couple of examples that already exist in the wilds of the internet with some generated data. Then, we'll dig into one particular clustering algorithm that looks promising. Finally, we'll look at that algorithm applied to some of our own data.\n",
    "\n",
    "To make this visualizable, we're going to work mostly in 2 dimensions. \n",
    "\n",
    "\n",
    "## What *aren't* we going to do?\n",
    "\n",
    "a.k.a. *\"great opportunities for future RST sessions\"*\n",
    "\n",
    "- have a deep dive into the **\"proper\" dimensionality** in which to do this work\n",
    "- have a deep dive into comparing algorithms on the basis of their **scaling** (we'll touch on it, and there is a nice write up linked later)\n",
    "- have a deep dive into **dimensionality reduction techniques** \n",
    "\n",
    "\n",
    "Ok, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(context='poster', style='darkgrid')\n",
    "sns.set_color_codes()\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn import datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo from `sklearn` \n",
    "\n",
    "1. Run the next cell, then come back.\n",
    "\n",
    "Let's start by pointing our eyeballs at the picture that pops out from running the script below. This is a lightly modified version of [the `sklearn` docs clustering demo](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html), and highlights the results of crossing a set of algorithms (columns) with a set of generated data sets (rows).\n",
    "\n",
    "In most cases, there are parameters that are necessary inputs and for these fits. These are magic numbers pulled from experience and thin air. For the most part, the parameters are even chosen to be \"correct\" for the data set e.g. k=2 for the annulus and crescent data sets, to illustrate a sort of \"best case\" scenario. In the lower righthand corner of each plot is the runtime of the fit step.\n",
    "\n",
    "The code is pretty involved but not particularly informative, so let's just look at the output graphic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes ~a minute to run\n",
    "%run sklearn-cluster-demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick take-aways:\n",
    "- speed?\n",
    "    - KMeans is fast\n",
    "    - AffinityPropagation, SpectralClustering, Ward, Agglomerative are slow\n",
    "- non-linear separation?\n",
    "    - SpectralClustering, Ward, Agglomerative, and DBSCAN seem to be capable \n",
    "    - the rest aren't\n",
    "- clusters not blob-like (esp. #4, the long skinny clusters)?\n",
    "    - even with \"correct\" k value, most algos fit a poor model!\n",
    "- \"null\" / no clusters?\n",
    "    - all bets are off; aka \"GIGO\" (garbage in garbage out)\n",
    "- best?\n",
    "    - *if I had to choose one algorithm based on this data alone, it looks like **DBSCAN** is the winner*\n",
    "\n",
    "There are some interesting patterns in there. The [sklearn user's guide](http://scikit-learn.org/stable/modules/clustering.html) (and associated links to algorithm details) has a nice table that briefly discusses the assumptions and use cases for the algorithms. I don't want to get into the derivations, though, so let's look at anothere data set and see what else we can surmise from empirical comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo from `HDBSCAN`\n",
    "\n",
    "The DBSCAN algorithm is included in the current release of sklearn, but the algorithm's authors subsequently [published an improved version](https://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14) to that algorithm, called HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) which addresses a handful of the shortcomings of their original DBSCAN.\n",
    "\n",
    "The paper's abstract: \n",
    "\n",
    "> We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a “flat” partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.\n",
    "\n",
    "The HDBSCAN docs have [a great writeup (and notebook)](https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html) that compares available Python clustering algorithms. This section is a modified version of that writeup.\n",
    "\n",
    "First, go get the data file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! wget https://github.com/scikit-learn-contrib/hdbscan/blob/master/notebooks/clusterable_data.npy?raw=true \\\n",
    "    -O clusterable_data.npy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('clusterable_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "# nb: because of the format of the .npy files, you'll see lots of transposing of data (data.T)\n",
    "plt.scatter(data.T[0], data.T[1], c='b', alpha=0.25, s=80)\n",
    "\n",
    "frame = plt.gca()\n",
    "frame.axes.get_xaxis().set_visible(False)\n",
    "frame.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important observations on this data:\n",
    "- there are a bunch of visually recognizable clusters\n",
    "- the shapes of the clusters varies\n",
    "- the core density of the clusters varies\n",
    "- there's a bunch of data scattered around the edges\n",
    "\n",
    "\n",
    "Let's line up some algorithms to compare. To make it simpler, this is a subset of the sklearn demo ones, plus HDBSCAN (which is not in sklearn). We're giving the algorithms some parameters (because they're required), and we'll come back to how we feel about them later.\n",
    "\n",
    "Below, we'll fit each of the algorithms to the dataset we've just looked at, and label the results according to the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tuples are:\n",
    "# (algorithm, algo args, algo kwargs)\n",
    "algo_combos = [\n",
    "    (cluster.KMeans,                  (),       {'n_clusters':6}),\n",
    "    (cluster.AffinityPropagation,     (),       {'preference':-5.0, 'damping':0.95}),\n",
    "    (cluster.MeanShift,               (0.175,), {'cluster_all':False}),\n",
    "    (cluster.SpectralClustering,      (),       {'n_clusters':6}),\n",
    "    (cluster.AgglomerativeClustering, (),       {'n_clusters':6, 'linkage':'ward'}),   \n",
    "    (cluster.DBSCAN,                  (),       {'eps':0.025}),\n",
    "    (hdbscan.HDBSCAN,                 (),       {'min_cluster_size':15})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 60, 'linewidths':0}\n",
    "\n",
    "# loop over algos and fit each one\n",
    "for i, (algo, args, algo_kwargs) in enumerate(algo_combos, start=1):\n",
    "    # catch the runtime for model fitting\n",
    "    start_time = time.time()\n",
    "    labels = algo(*args, **algo_kwargs).fit_predict(data)\n",
    "    end_time = time.time()\n",
    "    # plot the results\n",
    "    palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    ax = fig.add_subplot(4,2,i)    \n",
    "    ax.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)    \n",
    "    ax.text(-0.5, \n",
    "            0.67,\n",
    "            '{}: {:.2f} s'.format(str(algo.__name__), (end_time - start_time)),\n",
    "            fontsize=20\n",
    "           )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, take a minute and look through these outputs. The format is similar: the algorithm name and the run time for the fit. So, what are our observations? \n",
    "\n",
    "This time, let's be guided by [the HDBScan docs](https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html) which do a really nice job of summarizing some interpretable comparison criteria for these algorithms. The \"Rules\" are below.\n",
    "\n",
    "\n",
    "## \"Rules for EDA clustering\" \n",
    "\n",
    "*this is a paraphrased version of the full doc*\n",
    "\n",
    "- **\"don't be wrong\"** (\"dbw\")\n",
    "    - ...conservative in it’s clustering; it should be willing to not assign points to clusters; it should not group points together unless they really are in a cluster    \n",
    "- **intuitive parameters**\n",
    "    - ...parameters need to be intuitive enough that you can hopefully set them without having to know a lot about your data\n",
    "- **stable clusters**\n",
    "    - ...run the algorithm twice with a different random initialization, you should expect to get roughly the same clusters back\n",
    "    - ...taking a different random sample shouldn’t radically change the resulting cluster structure (unless your sampling has problems)\n",
    "    - ...when you vary the clustering algorithm parameters you want the clustering to change in a somewhat stable predictable fashion\n",
    "- **performance**\n",
    "    - ...you need a clustering algorithm that can scale to large data sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms evaluated w.r.t. \"Rules\"\n",
    "\n",
    "Both the [sklearn docs](http://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods) and the HDBScan docs have commentary on the assumptions that go into these algorithms, and where the pros and cons are. To learn more about the specific implementation (and assumptions) about each algorithm e.g. [KMeans](http://scikit-learn.org/stable/modules/clustering.html#k-means) the docs are incredibly detailed and worth a read. \n",
    "\n",
    "But for this session, we don't have time to become an expert in all of them. Instead, we'll aim to combine and simplify the comments from the two sources. Below is an informal, emoji-qualified verdict about each of the algorithms.\n",
    "\n",
    "- **KMeans**\n",
    "    - dbw: partitions every point (❌ ); assumes clusters are convex/globular (❌ )\n",
    "    - intuitive: have to pick k, or survey and select (❌ )\n",
    "    - stable: data-dependent, but initialization is random (⚠️ ) \n",
    "    - performance: very fast (✅ )\n",
    "    \n",
    "    \n",
    "- **AffinityPropagation**\n",
    "    - dbw: partitions (❌ ); convex/globular clusters (❌ )\n",
    "    - intuitive: trade \"k\" for two other, difficult-to-choose parameters (❌ )\n",
    "    - stable: deterministic over runs (✅ )\n",
    "    - performance: very slow, nearly intractable on >medium datasets (❌ )    \n",
    "    \n",
    "    \n",
    "- **MeanShift**\n",
    "    - dbw: clusters instead of partitions using KDE (✅ ); globular clusters (❌ )\n",
    "    - intuitive: KDE bandwidth slightly more intuitive than cluster count (⚠️ )\n",
    "    - stable: random intialization, heavily dependent on bandwidth choice (❌ )\n",
    "    - performance: good in principle, bad in sklearn implementation (❌ )\n",
    "    \n",
    "    \n",
    "- **SpectralClustering**\n",
    "    - dbw: partitions (❌ ); doesn't assume convex/globular clusters (✅ )\n",
    "    - intuitive: have to pick k (space xform + kmeans) (❌ )\n",
    "    - stable: slightly better than kmeans (⚠️ ) \n",
    "    - performance: space transform (manifold learning) before kmeans (❌ )\n",
    "    \n",
    "    \n",
    "- **AgglomerativeClustering**\n",
    "    - dbw: partitions (❌ ); doesn't assume convex/globular clusters (✅ )\n",
    "    - intuitive: have to pick k, or survey and select (❌ )\n",
    "    - stable: consistent over runs (✅ )\n",
    "    - performance: fast evaluation (✅ )\n",
    "    \n",
    "    \n",
    "- **DBSCAN**\n",
    "    - dbw: density-based, doesn't partition / allows \"noise\" (✅ ); doesn't assume convex/globular clusters (✅ )\n",
    "    - intuitive: distance metric (⚠️ ), minimum density threshold (✅ )\n",
    "    - stable: stable across runs, but not hyperparameters (❌ )\n",
    "    - performance: varying densities are challenging (⚠️ ); fast evaluation (✅ )\n",
    "    \n",
    "    \n",
    "- **HDBSCAN**\n",
    "    - dbw: density-based, doesn't partition / allows \"noise\" (✅ ); doesn't assume convex/globular clusters (✅ )\n",
    "    - intuitive: replace one unintuitive param for \"minimum cluster population\" and another questionable one (⚠️ )\n",
    "    - stable: over sparse subsamples (✅ ) and hyperparam choices (✅ )\n",
    "    - performance: (✅ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDBSCAN has a couple of excellent points for it, but I think the ones that I find most compelling are:\n",
    "1. no partitioning (allows noise)\n",
    "2. density-based (no globular assumptions)\n",
    "3. shifts a parameter selection from abstract notion (k) to a problem-specific one (how many points do you care about?)\n",
    "\n",
    "Let's take a closer look.\n",
    "\n",
    "# HDBSCAN\n",
    "\n",
    "The algo does have some kwargs, but only two seem to be important. The main one is \"what's the minimum cluster population that you would care about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify this main, default param to discover larger clusters\n",
    "hdbs = hdbscan.HDBSCAN(min_cluster_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn-ish api\n",
    "labels = hdbs.fit_predict(data)\n",
    "\n",
    "# the labels are arbitrary integers, -1 is \"noise\"\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up colors for plotting\n",
    "color_palette = sns.color_palette('deep', len(np.unique(labels)))\n",
    "\n",
    "# color assigned points, leave noise points as gray\n",
    "cluster_colors = [color_palette[x] if x >= 0 \n",
    "                  else (0.5, 0.5, 0.5) \n",
    "                  for x in labels]\n",
    "\n",
    "# draw the figure\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(*data.T, s=50, c=cluster_colors, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "This would probably be a good stand-alone RST. [Here's a pretty detailed answer](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html), and [here's an even more detailed answer](https://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14). Here's a very brief summary based on my read:\n",
    "\n",
    "- **Transform the space according to the density/sparsity.**\n",
    "    - *The core of the clustering algorithm is single linkage clustering, and it can be quite sensitive to noise.*\n",
    "    - roughly: focus on the dense cluster by exaggerating the spacing around lower-density points, calculate a particular point-wise distance metric       \n",
    "\n",
    "\n",
    "- **Build the minimum spanning tree of the distance weighted graph.**\n",
    "    - *we build the tree one edge at a time, always adding the lowest weight edge that connects the current tree to a vertex not yet in the tree.*\n",
    "    - roughly: construct a graph of all points with the minimum number of edges (weights are the above distance metric)\n",
    "\n",
    "\n",
    "- **Construct a cluster hierarchy of connected components.**\n",
    "    - *sort the edges of the tree by distance (in increasing order) and then iterate through, creating a new merged cluster for each edge.*\n",
    "    - roughly: calculate hierarchy (combination) of points into clusters, starting with smallest edges\n",
    "\n",
    "\n",
    "- **Condense the cluster hierarchy based on minimum cluster size.**\n",
    "    - *walk through the hierarchy and at each split ask if one of the new clusters created by the split has fewer points than the minimum cluster size.*\n",
    "    - roughly: test the effect of splitting clusters (compare to chosen min size)\n",
    "\n",
    "\n",
    "- **Extract the stable clusters from the condensed tree.**\n",
    "    - roughly: start with all points as individual clusters, calculate stability as function of lambda (inverse distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you understood all of the points above and you wanted to do some deeper inspection into the algorithm calculations. Fortunately, this implementation provides you with a couple tools for that!\n",
    "\n",
    "- linkage trees (#2 in above \"Summary\")\n",
    "    - methods for plotting, exporting data\n",
    "- condensed trees (#4/5 in above \"Summary\")\n",
    "\n",
    "Since I don't deeply understand the algorithm at this point, I'm not going to dwell on these. But to illustrate some of the perspective they convey, take the \"condensed tree\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "hdbs.condensed_tree_.plot(\n",
    "    # highlight the chosen clusters (by color)\n",
    "    select_clusters=True, selection_palette=color_palette\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda is an inverse distance metric threshold - it represents that pairwise distance matrix referred to in the steps above. As lambda goes from 0 (pairwise distance ~infinity) to larger numbers (small pairwise distance), the splitting lines represent how \"clusters\" are determined to split off. \n",
    "\n",
    "Uncomment the `select_clusters` kwarg and you can see how this chart maps onto the actual colored clusters plotted in the data space. Some observations:\n",
    "- dark blue = cluster is very unique, stays separate the whole time\n",
    "- red and purple separate late in the process\n",
    "- purple has lots of interesting structure that didn't \"make it\" into a cluster\n",
    "\n",
    "If you wanted to do some additional analysis on the interim steps, there are also some helper methods for getting data conveniently into other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also: to_networkx(), to_numpy()\n",
    "df = hdbs.condensed_tree_.to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing parameters\n",
    "\n",
    "One of the values here was not having to \"choose k.\" Instead, the primary parameter is `min_cluster_size`. The docs summarize this as: \n",
    "> set it to the smallest size grouping that you wish to consider a cluster. \n",
    "\n",
    "which I think is a nicer framing of the parameter selection problem than choosing k in kmeans!\n",
    "\n",
    "There is one other parameter that has a strong effect. The `min_samples` parameter is\n",
    "\n",
    "> a measure of how conservative you want you clustering to be. \n",
    "\n",
    "Roughly, larger `min_samples` is more conservative clustering. The default value is `min_samples = min_cluster_size` (maximally conservative), and you can adjust it down to taste.\n",
    "\n",
    "Let's look at the effect."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mcs = 50\n",
    "ms = 50\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, \n",
    "                            min_samples=ms   \n",
    "                           ).fit(data)\n",
    "\n",
    "color_palette = sns.color_palette('deep', 12)\n",
    "cluster_colors = [color_palette[x] if x >= 0\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in clusterer.labels_]\n",
    "# saturate by probabilities\n",
    "#cluster_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(*data.T, s=50, linewidth=0, c=cluster_colors, alpha=0.25)\n",
    "plt.title('min_cluster_size: {}; min_samples: {}'.format(mcs, ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller = more \"aggressive\" clustering (i.e. \"label fewer points as noise\")\n",
    "min_sample_vals = [1,10,20,50]\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 60, 'linewidths':0}\n",
    "\n",
    "for i, minsamp in enumerate(min_sample_vals, start=1):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=minsamp).fit(data)\n",
    "    color_palette = sns.color_palette('deep', len(clusterer.labels_))    \n",
    "    cluster_colors = [color_palette[x] if x >= 0\n",
    "                      else (0.5, 0.5, 0.5)\n",
    "                      for x in clusterer.labels_]\n",
    "    # saturate by probabilities\n",
    "    #cluster_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
    "\n",
    "    ax = fig.add_subplot(2,2,i)    \n",
    "    ax.scatter(data.T[0], data.T[1], c=cluster_colors, **plot_kwds)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)    \n",
    "    ax.text(-0.5, 0.67, 'min_samples: {}'.format(minsamp), fontsize=20)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning (predicting) new points\n",
    "\n",
    "What if we wanted to train and pin a model, and then use it to predict cluster assignment for some new points?\n",
    "\n",
    "Recall that training the HDBSCAN model is dependent on the underlying density, so it only makes sense to do this in a  limited quantity before retraining based on all the currently-available data (that is, recalculate the trees and clusters).\n",
    "\n",
    "There are some extra calculations (and caching) done to speed this process up, so you can either trigger them with a kwarg at instantiation, or call one of the model's functions to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdbs = hdbscan.HDBSCAN(min_cluster_size=20, prediction_data=True).fit(data)\n",
    "\n",
    "# or on an existing, trained model\n",
    "#hdbs.generate_prediction_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some uniformly random data\n",
    "test_points = np.random.random(size=(50, 2)) - 0.5\n",
    "\n",
    "color_palette = sns.color_palette('deep', 12)\n",
    "cluster_colors = [sns.desaturate(color_palette[col], sat) \n",
    "                      for col, sat in zip(hdbs.labels_,hdbs.probabilities_)]\n",
    "# nb: we haven't talked much about them but each point is also assigned a probability score \n",
    "#  which is ~ \"how sure are we that this point belongs in this cluster\"\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.scatter(data.T[0], data.T[1], c=cluster_colors, **plot_kwds);\n",
    "plt.scatter(*test_points.T, c='k', s=60, label='new points')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the approximate_predict() method is module-level, and takes the model object as an arg\n",
    "test_labels, strengths = hdbscan.approximate_predict(hdbs, test_points)\n",
    "\n",
    "# show the predicted assignments\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color points that are assigned clusters, leave noise points black\n",
    "test_colors = [color_palette[col] if col >= 0 else (0.1, 0.1, 0.1) for col in test_labels]\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "# training data\n",
    "plt.scatter(data.T[0], data.T[1], c=cluster_colors, **plot_kwds);\n",
    "# new data points\n",
    "plt.scatter(*test_points.T, c=test_colors, s=60, linewidths=1, edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet data\n",
    "\n",
    "\n",
    "- filtered \"eclipse\" tweets, extracted text  \n",
    "- CountVectorizer (TweetTokenizer, 1-grams, drop 0.1% long tail tokens)\n",
    "- SVD to 50D \n",
    "- tSNE (2d) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download from e.g. GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data matrix\n",
    "tw_data = np.load('2d-eclipse-tweets.npy')\n",
    "\n",
    "# load tweet text (write out + read in original file in pd.dataframe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot raw data\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(tw_data.T[0], data.T[1], c='b', alpha=0.25, s=80)\n",
    "\n",
    "frame = plt.gca()\n",
    "frame.axes.get_xaxis().set_visible(False)\n",
    "frame.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit cluster model\n",
    "hdbs = hdbscan.HDBSCAN(min_cluster_size=20, prediction_data=True).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
