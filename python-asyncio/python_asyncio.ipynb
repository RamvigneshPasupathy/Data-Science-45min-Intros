{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coroutines for IO-bound tasks\n",
    "\n",
    "In this notebook, we'll weave together our new (Tweet Parser)[https://github.com/tw-ddis/tweet_parser] and some python asyncio magic.\n",
    "\n",
    "Let's set up the environment and demonstrate a motivating example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dD9NgzLhbBM\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dD9NgzLhbBM\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tweet_parser.tweet import Tweet\n",
    "\n",
    "import sec # you will not have this python file; I use it to keep `secrets` like passwords hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a few constants here that will be used throughout our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = \"agonzales@twitter.com\"\n",
    "AUTH = requests.auth.HTTPBasicAuth(username, sec.GNIP_API_PW)\n",
    "GNIP_BASE_URL = \"https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a little helper for programatically generating valid queries for terms with the Gnip api. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_query_url(url, terms, max_results=100):\n",
    "    if isinstance(terms, str):\n",
    "        terms = terms.split()\n",
    "    return ''.join([url,\n",
    "                    \"query=\",\n",
    "                    \"%20\".join(terms),\n",
    "                    \"&maxResults={}\".format(max_results)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say you want to get a collection of tweets matching some criteria - this is an extremely common task. The process might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?query=just%20bought%20a%20house&maxResults=100\n"
     ]
    }
   ],
   "source": [
    "query = gen_query_url(GNIP_BASE_URL, [\"just\", \"bought\", \"a\", \"house\"])\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def sync_tweets(query):\n",
    "    return requests.get(url=query, auth=AUTH).json()['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.9 ms, sys: 8.19 ms, total: 52.1 ms\n",
      "Wall time: 805 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets = [Tweet(i) for i in sync_tweets(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus, my best friend in London just bought a house with her boyfriend why am I feeling so oldddðŸ˜­\n"
     ]
    }
   ],
   "source": [
    "print(tweets[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy peasy. What if you have a bunch of queries to match (this is a bit contrived, but serves a purpose). You might define all your queries as such and run a for loop to query all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?query=eclipse&maxResults=100',\n",
       " 'https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?query=nuclear&maxResults=100',\n",
       " 'https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?query=korea&maxResults=100',\n",
       " 'https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?query=cats&maxResults=100',\n",
       " 'https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?query=ai&maxResults=100',\n",
       " 'https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?query=memes&maxResults=100',\n",
       " 'https://gnip-api.twitter.com/search/30day/accounts/shendrickson/peabody.json?query=googlebro&maxResults=100']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formed_query = partial(gen_query_url, url=GNIP_BASE_URL, max_results=100)\n",
    "queries = [formed_query(terms=[i]) for i in [\"eclipse\", \"nuclear\", \"korea\", \"cats\", \"ai\", \"memes\", \"googlebro\"]]\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 335 ms, sys: 28.1 ms, total: 363 ms\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets = [Tweet(i) for i in it.chain.from_iterable([sync_tweets(query) for query in queries])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works great, but notice that there seems to be linear scaling for the time it takes to run this. Given that this is a trivial amount of _computation_ and a task that is almost entirely taken up by system calls / IO, it's a perfect opportunity to add parallism to the mix and speed it up.\n",
    "\n",
    "IO-bound parallism is commonly handled with a technique called asyncronous programming, in which the semantics _coroutine_, _event loop_, _user-level thread_, _task_, _future_, etc. are introduced. \n",
    "\n",
    "In modern python (>3.5), the language has builtins for using coroutines, exposed via the `asyncio` module and the keywords `async` and `await`. Several libraries have been introduced that make use of coroutines internally, such as `aiohttp`, which is mostly a coroutine verison of `requests`.\n",
    "\n",
    "\n",
    "Let's look at what the basic coroutine version of our above simple example would look like in aiohttp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import async_timeout\n",
    "\n",
    "    \n",
    "async def fetch_tweets_coroutine(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url, auth=aiohttp.BasicAuth(AUTH.username, AUTH.password)) as response:\n",
    "            return await response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.3 ms, sys: 4.15 ms, total: 44.5 ms\n",
      "Wall time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loop = asyncio.get_event_loop()\n",
    "tweets = [Tweet(i) for i in loop.run_until_complete(fetch_tweets_coroutine(query))['results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2701385078 Jesus, my best friend in London just bought a house with her boyfriend why am I feeling so oldddðŸ˜­\n"
     ]
    }
   ],
   "source": [
    "print(tweets[0].user_id, tweets[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a lot more code that our simple requests example and doesn't work any more quickly, though this is expected since the time is really response time to and from Gnip. \n",
    "\n",
    "Let's try again with our longer set of queries, redefining the methods to handle this more naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_tweets_fancy(session, url):\n",
    "    async with session.get(url, auth=aiohttp.BasicAuth(AUTH.username, AUTH.password)) as response:\n",
    "        # print(\"collecting query: {}\".format(url))\n",
    "        _json = await response.json()\n",
    "        return [Tweet(t) for t in _json[\"results\"]]\n",
    "    \n",
    "    \n",
    "async def collect_queries(queries):\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for query in queries:\n",
    "            task = asyncio.ensure_future(fetch_tweets_fancy(session, query))\n",
    "            tasks.append(task)\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formed_query = partial(gen_query_url, url=GNIP_BASE_URL, max_results=100)\n",
    "queries = [formed_query(terms=[i]) for i in [\"eclipse\", \"nuclear\", \"korea\", \"cats\", \"ai\", \"memes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 135 ms, sys: 12.3 ms, total: 147 ms\n",
      "Wall time: 758 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loop = asyncio.get_event_loop()\n",
    "future = asyncio.ensure_future(collect_queries(queries))\n",
    "res = list(it.chain.from_iterable(loop.run_until_complete(future)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @surly_viking: BAHAHAHAHA!!! Mom Asks To â€˜Rescheduleâ€™ Eclipse Because Itâ€™s On A School Day, Internet Canâ€™t Stop Laughing https://t.co/Cqâ€¦\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "print(res[0].text)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what the hell is a coroutine and why does it work the way that it does?\n",
    "\n",
    "\n",
    "First, we have to talk about the differences between threads, coroutines, parallelism, and concurrency.\n",
    "For a short intro, see (this SO thread)[https://stackoverflow.com/questions/1934715/difference-between-a-coroutine-and-a-thread]\n",
    "\n",
    "- Concurrency - separation of tasks for seamless execution (IDE, operating systems, complex ops)\n",
    "\n",
    "- parallelism - execution of multiple tasks simultaneously to increase speed\n",
    "\n",
    "- thread - OS level scheduling and concurrency. blocking, context switching, deadlocks, lock contention, kernel, premption\n",
    "\n",
    "- ULT / Coroutine - non-blocking, program-level, event-based, \"juggling\"\n",
    "\n",
    "\n",
    "(discussion)\n",
    "\n",
    "\n",
    "further reading:\n",
    "\n",
    "- (coroutine in node.js and python)[http://sahandsaba.com/understanding-asyncio-node-js-python-3-4.html]\n",
    "\n",
    "- (combinatorics and coroutines)[http://sahandsaba.com/combinatorial-generation-using-coroutines-in-python.html]\n",
    "\n",
    "- (Long example of python internals / building coroutines)[http://www.aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html]\n",
    "\n",
    "- (making 1 million requests with aiohttp)[https://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html]\n",
    "\n",
    "- (coroutine patterns)[https://medium.com/python-pandemonium/asyncio-coroutine-patterns-beyond-await-a6121486656f]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
