{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "There is a perception that Twitter data can be used to surface insights: unexpected features of the data that have business value. In this tutorial, I will explore some of the difficulties and opportunities of turning that perception into reality. \n",
    "\n",
    "We will focus exclusively on _text_ analysis, and on insights represented by textual differences between documents and corpora. We will start by constructing a small, simple data set that represents a few notions of what insights _should_ be surfaced. We can then examine which technique uncover which insights.\n",
    "\n",
    "Next, we will move to real data, where we don't know what we might surface. We will have to address data cleaning and curation, both at the beginning and in an iterative fashion as our insights-generation surfaces artifacts of insufficient data curation. We will finish by developing and evaluating a variety of tools and techiques for comparing text-based data.\n",
    "\n",
    "## Resources\n",
    "\n",
    "Good further reading, and the source of some of the ideas here:\n",
    "https://de.dariah.eu/tatom/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires Python 3.6 or greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Synthetic Example\n",
    "\n",
    "Let's build some intuition by creating two artificial documents, which represent textual differences that we might intend to surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0,doc1 = ('bun cat cat dog bird','bun cat dog dog dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of unigram frequency, here are 3 differences:\n",
    "* 1 more \"cat\" in doc0 than in doc1\n",
    "* 2 more \"dog\" in doc1 than in doc0\n",
    "* \"bird\" only exists in doc0\n",
    "\n",
    "Let's throw together a function that prints out the differences in term frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(doc0,doc1,vectorizer):\n",
    "    \"\"\"\n",
    "    print difference in absolute term-frequency difference for each unigram\n",
    "    \"\"\"\n",
    "    tf = vectorizer.fit_transform([doc0,doc1])\n",
    "    # this is a 2-column matrix, where the columns represent doc0 and doc1\n",
    "    tfa = tf.toarray()\n",
    "    # make tuples of the tokens and the difference of their doc0 and doc1 coefficients\n",
    "    # if we use a basic token count vectorizer, this is the term frequency difference \n",
    "    tup = zip(vectorizer.get_feature_names(),tfa[0] - tfa[1])\n",
    "    # print the top-10 tokens ranked by the difference measure\n",
    "    for token,score in list(reversed(sorted(tup,key=operator.itemgetter(1))))[:10]:\n",
    "        print(token,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(doc0,doc1,CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* positive numbers are more \"doc0-like\"\n",
    "* the \"dog\" score is higher in absolute value than the bird score\n",
    "* \"bird\" and \"cat\" are indistinguishable\n",
    "\n",
    "Let's try inverse-document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(doc0,doc1,TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* \"bird\" now has a larger coefficient that \"cat\"\n",
    "* \"dog is still most significant that \"cat\"\n",
    "\n",
    "How does this scale?\n",
    "\n",
    "Let's construct:\n",
    "* doc0 is +1 \"cat\"\n",
    "* doc0 is +40 \"bun\"\n",
    "* doc0 is +1 \"bird\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0 = 'cat '*5 + 'dog '*3 + 'bun '*350 + 'bird '\n",
    "doc1 = 'cat '*4 + 'dog '*3 + 'bun '*310 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(doc0,doc1,CountVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(doc0,doc1,TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* \"bird\" stands out strongly\n",
    "* \"cat\" and \"dog\" are similar in absolute value\n",
    "* \"bun\" is the least significant token\n",
    "\n",
    "What about including 2-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(doc0,doc1,TfidfVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's impossible to read. Let's build better formatting into our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(doc0,doc1,vectorizer):\n",
    "    tf = vectorizer.fit_transform([doc0,doc1])\n",
    "    tfa = tf.toarray()\n",
    "    tup = zip(vectorizer.get_feature_names(),tfa[0] - tfa[1])\n",
    "    \n",
    "    # print \n",
    "    max_token_length = 0\n",
    "    output_tuples = list(reversed(sorted(tup,key=operator.itemgetter(1))))[:10]\n",
    "\n",
    "    for token,score in output_tuples:\n",
    "        if max_token_length < len(token):\n",
    "            max_token_length = len(token)\n",
    "    for token,score in output_tuples:\n",
    "        print(f\"{token:{max_token_length}s} {score:.3e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(doc0,doc1,TfidfVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* grams with \"bird\" still stand out\n",
    "* scores are getting hard to interpret\n",
    "\n",
    "Let's get some real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from tweet_parser.tweet import Tweet\n",
    "from searchtweets import (ResultStream,\n",
    "                           collect_results,\n",
    "                           gen_rule_payload,\n",
    "                           load_credentials)\n",
    "\n",
    "search_args = load_credentials(filename=\"~/.twitter_keys.yaml\",\n",
    "                               account_type=\"enterprise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pats_rule = \"#patriots OR @patriots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eagles_rule = \"#eagles OR @eagles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date=\"2018-01-28\"\n",
    "to_date=\"2018-01-29\"\n",
    "max_results = 3000\n",
    "\n",
    "pats_rule = gen_rule_payload(_pats_rule,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        )\n",
    "eagles_rule = gen_rule_payload(_eagles_rule,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_results_list = collect_results(eagles_rule, \n",
    "                               max_results=max_results, \n",
    "                               result_stream_args=search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats_results_list = collect_results(pats_rule, \n",
    "                               max_results=max_results, \n",
    "                               result_stream_args=search_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join all tweet bodies in a corpus into one space-delimited document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_body_text = [tweet['body'] for tweet in eagles_results_list]\n",
    "eagles_doc = ' '.join(eagles_body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats_body_text = [tweet['body'] for tweet in pats_results_list]\n",
    "pats_doc = ' '.join(pats_body_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data (AS YOU ALWAYS SHOULD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_body_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew...this is gonna take some cleaning.\n",
    "\n",
    "Let's start with a tokenizer and a stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    stop_words=stopwords,\n",
    "    ngram_range=(1,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 10 1- and 2-grams for the Eagles corpus/document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(eagles_doc,pats_doc,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the ability to specify `n` in top-`n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_docs(doc0,doc1,vectorizer,n_to_display=10):\n",
    "    tfm_sparse = vectorizer.fit_transform([doc0,doc1])\n",
    "    tfm = tfm_sparse.toarray()\n",
    "    tup = zip(vectorizer.get_feature_names(),tfm[0] - tfm[1])\n",
    "    \n",
    "    # print \n",
    "    max_token_length = 0\n",
    "    output_tuples = list(reversed(sorted(tup,key=operator.itemgetter(1))))[:n_to_display]\n",
    "\n",
    "    for token,score in output_tuples:\n",
    "        if max_token_length < len(token):\n",
    "            max_token_length = len(token)\n",
    "    for token,score in output_tuples:\n",
    "        print(f\"{token:{max_token_length}s} {score:.3e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_docs(eagles_doc,pats_doc,vectorizer,n_to_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_docs(pats_doc,eagles_doc,vectorizer,n_to_display=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't really evalute more sophisticated text comparison techniques without doing better filtering on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add token filtering to the TweetTokenizer\n",
    "def filter_tokens(token):\n",
    "    if len(token) < 2:\n",
    "        return False\n",
    "    if token.startswith('http'):\n",
    "        return False\n",
    "    if '’' in token:\n",
    "        return False\n",
    "    if '…' in token or '...' in token:\n",
    "        return False\n",
    "    return True\n",
    "def custom_tokenizer(doc):\n",
    "    initial_tokens = tokenizer.tokenize(doc)\n",
    "    return [token for token in initial_tokens if filter_tokens(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words=stopwords,\n",
    "    ngram_range=(1,2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_docs(eagles_doc,pats_doc,vectorizer,n_to_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_docs(pats_doc,eagles_doc,vectorizer,n_to_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retweets makes a mess of a term frequency analysis on documents consisting of concatenated tweet bodies. Remove them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_body_text_noRT = [tweet['body'] for tweet in eagles_results_list if tweet['verb'] == 'post']\n",
    "eagles_doc_noRT = ' '.join(eagles_body_text_noRT)\n",
    "\n",
    "pats_body_text_noRT = [tweet['body'] for tweet in pats_results_list if tweet['verb'] == 'post']\n",
    "pats_doc_noRT = ' '.join(pats_body_text_noRT)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words=stopwords,\n",
    "    ngram_range=(1,2),\n",
    ")\n",
    "\n",
    "compare_docs(eagles_doc_noRT,pats_doc_noRT,vectorizer,n_to_display=20)\n",
    "print(\"\\n\")\n",
    "compare_docs(pats_doc_noRT,eagles_doc_noRT,vectorizer,n_to_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, now we have clear evidence of the political notion of the \"#patriots\" clause in our rule. Let's simplfy things by removing the hashtags from the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pats_rule = \"@patriots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eagles_rule = \"@eagles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date=\"2018-01-28\"\n",
    "to_date=\"2018-01-29\"\n",
    "max_results = 20000\n",
    "\n",
    "pats_rule = gen_rule_payload(_pats_rule,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        )\n",
    "eagles_rule = gen_rule_payload(_eagles_rule,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_results_list = collect_results(eagles_rule, \n",
    "                               max_results=max_results, \n",
    "                               result_stream_args=search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats_results_list = collect_results(pats_rule, \n",
    "                               max_results=max_results, \n",
    "                               result_stream_args=search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_body_text_noRT = [tweet['body'] for tweet in eagles_results_list if tweet['verb'] == 'post']\n",
    "eagles_doc_noRT = ' '.join(eagles_body_text_noRT)\n",
    "\n",
    "pats_body_text_noRT = [tweet['body'] for tweet in pats_results_list if tweet['verb'] == 'post']\n",
    "pats_doc_noRT = ' '.join(pats_body_text_noRT)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words=stopwords,\n",
    "    ngram_range=(1,2),\n",
    ")\n",
    "\n",
    "compare_docs(eagles_doc_noRT,pats_doc_noRT,vectorizer,n_to_display=20)\n",
    "print(\"\\n\")\n",
    "compare_docs(pats_doc_noRT,eagles_doc_noRT,vectorizer,n_to_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things we could do:\n",
    "* vectorize tweets as documents, and summarize or aggregate the coeeficients \n",
    "* select tokens for which the mean coefficient within a corpus is zero\n",
    "* look at the difference in mean coefficient\n",
    "\n",
    "Let's start by going back to simple corpora, and account for individual docs this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus0 = [\"cat\",\"cat dog\"]\n",
    "corpus1 = [\"bun\",\"dog\",\"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic unigram vectorizer with Twitter-specific tokenization and stopwords\n",
    "vectorizer = CountVectorizer(\n",
    "                    tokenizer=custom_tokenizer,\n",
    "                    stop_words=stopwords,\n",
    "                    ngram_range=(1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the term-frequency matrix\n",
    "m = vectorizer.fit_transform(corpus0+corpus1)\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print(vocab)\n",
    "\n",
    "m = m.toarray()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get TF matrices for each corpus\n",
    "corpus0_indices = range(len(corpus0))\n",
    "corpus1_indices = range(len(corpus0),len(corpus0)+len(corpus1))\n",
    "m0 = m[corpus0_indices,:]\n",
    "m1 = m[corpus1_indices,:]\n",
    "print(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average term frequency within each corpus\n",
    "c0_means = np.mean(m0,axis=0)\n",
    "c1_means = np.mean(m1,axis=0)\n",
    "print(c0_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the indices of the distinct tokens, which only occur in a single corpus\n",
    "distinct_indices = c0_means * c1_means == 0\n",
    "print(vocab[distinct_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now remove the distinct tokens from the vocab\n",
    "print(m[:, np.invert(distinct_indices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate things\n",
    "m0_non_distinct = m[:, np.invert(distinct_indices)][corpus0_indices,:]\n",
    "m1_non_distinct = m[:, np.invert(distinct_indices)][corpus1_indices,:]\n",
    "c0_non_distinct_means = np.mean(m0_non_distinct,axis=0)\n",
    "c1_non_distinct_means = np.mean(m1_non_distinct,axis=0)\n",
    "# and take the difference\n",
    "print(c0_non_distinct_means - c1_non_distinct_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference in averages is sometimes called \"keyness\".\n",
    "\n",
    "Now let's do it on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and identify the corpora\n",
    "docs = eagles_body_text_noRT + pats_body_text_noRT\n",
    "eagles_indices = range(len(eagles_body_text_noRT))\n",
    "pats_indices = range(len(eagles_body_text_noRT),len(eagles_body_text_noRT) + len(pats_body_text_noRT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a single vectorizer because we care about the joint vocabulary\n",
    "vectorizer = CountVectorizer(\n",
    "                    tokenizer=custom_tokenizer,\n",
    "                    stop_words=stopwords,\n",
    "                    ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "dtm = vectorizer.fit_transform(docs).toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "eagles_dtm = dtm[eagles_indices, :]\n",
    "pats_dtm = dtm[pats_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the average coefficient for each vocab element, for each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for every token in the vocab; rows for tweets in the corpus\n",
    "eagles_means = np.mean(eagles_dtm,axis=0)\n",
    "pats_means = np.mean(pats_dtm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by looking for _distinct_ tokens, which only exist in one corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices for any column with zero mean in either corpus\n",
    "distinct_indices = eagles_means * pats_means == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(np.count_nonzero(distinct_indices)) + \" distinct tokens out of \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_ranking = np.argsort(eagles_means[distinct_indices])[::-1]\n",
    "pats_ranking = np.argsort(pats_means[distinct_indices])[::-1]\n",
    "total_ranking = np.argsort(eagles_means[distinct_indices] + pats_means[distinct_indices])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[distinct_indices][total_ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top distinct Eagles tokens by average term count in Eagles corpus\")\n",
    "for token in vocab[distinct_indices][eagles_ranking][:10]:\n",
    "    print_str = f\"{token:30s} {eagles_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "    print(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top distinct Patriots tokens by average term count in Patriots corpus\")\n",
    "for token in vocab[distinct_indices][pats_ranking][:10]:\n",
    "    print_str = f\"{token:30s} {pats_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "    print(print_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this change if we account for inverse document frequency?\n",
    "\n",
    "Let's build a function and encapsulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_corpora(corpus0,corpus1,vectorizer,n_to_display=10):\n",
    "    corpus0_indices = range(len(corpus0))\n",
    "    corpus1_indices = range(len(corpus0), len(corpus0) + len(corpus1))\n",
    "    m_sparse = vectorizer.fit_transform(corpus0 + corpus1)\n",
    "    m = m_sparse.toarray()\n",
    "\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    m_corpus0 = m[corpus0_indices,:]\n",
    "    m_corpus1 = m[corpus1_indices,:]\n",
    "    \n",
    "    corpus0_means = np.mean(m_corpus0,axis=0)\n",
    "    corpus1_means = np.mean(m_corpus1,axis=0)\n",
    "    \n",
    "    distinct_indices = corpus0_means * corpus1_means == 0\n",
    "    print(str(np.count_nonzero(distinct_indices)) + \" distinct tokens out of \" + str(len(vocab)) + '\\n')    \n",
    "    \n",
    "    corpus0_ranking = np.argsort(corpus0_means[distinct_indices])[::-1]\n",
    "    corpus1_ranking = np.argsort(corpus1_means[distinct_indices])[::-1]\n",
    "\n",
    "    print(\"Top distinct tokens from corpus0 by average term count in corpus\")\n",
    "    for token in vocab[distinct_indices][corpus0_ranking][:n_to_display]:\n",
    "        print_str = f\"{token:30s} {corpus0_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "        print(print_str)\n",
    "    print()\n",
    "    print(\"Top distinct tokens from corpus1 by average term count in corpus\")\n",
    "    for token in vocab[distinct_indices][corpus1_ranking][:n_to_display]:\n",
    "        print_str = f\"{token:30s} {corpus1_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "        print(print_str)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(\n",
    "vectorizer = CountVectorizer(\n",
    "                    tokenizer=custom_tokenizer,\n",
    "                    stop_words=stopwords,\n",
    "                    ngram_range=(1,1)\n",
    ")\n",
    "compare_corpora(eagles_body_text_noRT,pats_body_text_noRT,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the distrinct tokens and look at the maximum _difference_ in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_corpora(corpus0,corpus1,vectorizer,n_to_display=10):\n",
    "    \n",
    "    # get corpus indices\n",
    "    corpus0_indices = range(len(corpus0))\n",
    "    corpus1_indices = range(len(corpus0), len(corpus0) + len(corpus1))\n",
    "    m_sparse = vectorizer.fit_transform(corpus0 + corpus1)\n",
    "    m = m_sparse.toarray()\n",
    "\n",
    "    # get vocab and TF matrices for each corpus\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    m_corpus0 = m[corpus0_indices,:]\n",
    "    m_corpus1 = m[corpus1_indices,:]\n",
    "    \n",
    "    corpus0_means = np.mean(m_corpus0,axis=0)\n",
    "    corpus1_means = np.mean(m_corpus1,axis=0)\n",
    "    \n",
    "    distinct_indices = corpus0_means * corpus1_means == 0\n",
    "    print(str(np.count_nonzero(distinct_indices)) + \" distinct tokens out of \" + str(len(vocab)) + '\\n')    \n",
    "    \n",
    "    corpus0_ranking = np.argsort(corpus0_means[distinct_indices])[::-1]\n",
    "    corpus1_ranking = np.argsort(corpus1_means[distinct_indices])[::-1]\n",
    "\n",
    "    print(\"Top distinct tokens from corpus0 by average term count in corpus\")\n",
    "    for token in vocab[distinct_indices][corpus0_ranking][:n_to_display]:\n",
    "        print_str = f\"{token:30s} {corpus0_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "        print(print_str)\n",
    "    print()\n",
    "    print(\"Top distinct tokens from corpus1 by average term count in corpus\")\n",
    "    for token in vocab[distinct_indices][corpus1_ranking][:n_to_display]:\n",
    "        print_str = f\"{token:30s} {corpus1_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "        print(print_str)    \n",
    "    \n",
    "    # remove distinct tokens\n",
    "    m = m[:, np.invert(distinct_indices)]\n",
    "    vocab = vocab[np.invert(distinct_indices)]\n",
    "    \n",
    "    # recalculate stuff\n",
    "    m_corpus0 = m[corpus0_indices,:]\n",
    "    m_corpus1 = m[corpus1_indices,:]\n",
    "    corpus0_means = np.mean(m_corpus0,axis=0)\n",
    "    corpus1_means = np.mean(m_corpus1,axis=0)\n",
    "    \n",
    "    # get \"keyness\"\n",
    "    keyness = corpus0_means - corpus1_means\n",
    "    # order token indices by keyness\n",
    "    ranking = np.argsort(keyness)[::-1]\n",
    "    \n",
    "    print()\n",
    "    print(\"Top tokens by keyness from corpus0 by average term count in corpus\")\n",
    "    for rank in ranking[:n_to_display]:\n",
    "        token = vocab[rank]\n",
    "        print_str = f\"{token:30s} {keyness[rank]:.3g}\"\n",
    "        print(print_str)       \n",
    "   \n",
    "    print()\n",
    "    print(\"Top tokens by keyness from corpus1 by average term count in corpus\")\n",
    "    for rank in ranking[-n_to_display:]:\n",
    "        token = vocab[rank]\n",
    "        print_str = f\"{token:30s} {keyness[rank]:.3g}\"\n",
    "        print(print_str)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "                    tokenizer=custom_tokenizer,\n",
    "                    stop_words=stopwords,\n",
    "                    ngram_range=(1,1)\n",
    ")\n",
    "compare_corpora(eagles_body_text_noRT,pats_body_text_noRT,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "* Distinct tokens are frequency players' handles or institutions specific to a team.\n",
    "* Terms counts and IDF surface different top keyness terms; would need to investigate.\n",
    "* Taking the mean occurence across the docs in a corpus might not be the best aggregation for sparse spaces like Twitter. \n",
    "\n",
    "Despite common intuition around surfacing insights through text analysis and token counting, there isn't One Way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
