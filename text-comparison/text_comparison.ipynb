{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "There is a perception that Twitter data can be used to surface insights: unexpected features of the data that have business value. In this tutorial, I will explore some of the difficulties and opportunities of turning that perception into reality. \n",
    "\n",
    "We will focus exclusively on _text_ analysis, and on insights represented by textual differences between documents and corpora. We will start by constructing a small, simple data set that represents a few notions of what insights _should_ be surfaced. We can then examine which technique uncover which insights.\n",
    "\n",
    "Next, we will move to real data, where we don't know what we might surface. We will have to address data cleaning and curation, both at the beginning and in an iterative fashion as our insights-generation surfaces artifacts of insufficient data curation. We will finish by developing and evaluating a variety of tools and techiques for comparing text-based data.\n",
    "\n",
    "## Resources\n",
    "\n",
    "Good further reading, and the source of some of the ideas here:\n",
    "https://de.dariah.eu/tatom/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires Python 3.6 or greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Synthetic Example\n",
    "\n",
    "Let's build some intuition by creating two artificial documents, which represent textual differences that we might intend to surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0,doc1 = ('bun cat cat dog bird','bun cat dog dog dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of unigram frequency, here are 3 differences:\n",
    "* 1 more \"cat\" in doc0 than in doc1\n",
    "* 2 more \"dog\" in doc1 than in doc0\n",
    "* \"bird\" only exists in doc0\n",
    "\n",
    "Let's throw together a function that prints out the differences in term frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(doc0,doc1,vectorizer):\n",
    "    \"\"\"\n",
    "    print difference in absolute term-frequency difference for each unigram\n",
    "    \"\"\"\n",
    "    tf = vectorizer.fit_transform([doc0,doc1])\n",
    "    # this is a 2-column matrix, where the columns represent doc0 and doc1\n",
    "    tfa = tf.toarray()\n",
    "    # make tuples of the tokens and the difference of their doc0 and doc1 coefficients\n",
    "    # if we use a basic token count vectorizer, this is the term frequency difference \n",
    "    tup = zip(vectorizer.get_feature_names(),tfa[0] - tfa[1])\n",
    "    # print the top-10 tokens ranked by the difference measure\n",
    "    for token,score in list(reversed(sorted(tup,key=operator.itemgetter(1))))[:10]:\n",
    "        print(token,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 1\n",
      "bird 1\n",
      "bun 0\n",
      "dog -2\n"
     ]
    }
   ],
   "source": [
    "func(doc0,doc1,CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* positive numbers are more \"doc0-like\"\n",
    "* the \"dog\" score is higher in absolute value than the bird score\n",
    "* \"bird\" and \"cat\" are indistinguishable\n",
    "\n",
    "Let's try inverse-document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird 0.4976748316029239\n",
      "cat 0.406688138613708\n",
      "bun 0.05258839701797219\n",
      "dog -0.5504342921375551\n"
     ]
    }
   ],
   "source": [
    "func(doc0,doc1,TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* \"bird\" now has a larger coefficient that \"cat\"\n",
    "* \"dog is still most significant that \"cat\"\n",
    "\n",
    "How does this scale?\n",
    "\n",
    "Let's construct:\n",
    "* doc0 is +1 \"cat\"\n",
    "* doc0 is +40 \"bun\"\n",
    "* doc0 is +1 \"bird\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0 = 'cat '*5 + 'dog '*3 + 'bun '*350 + 'bird '\n",
    "doc1 = 'cat '*4 + 'dog '*3 + 'bun '*310 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bun 40\n",
      "cat 1\n",
      "bird 1\n",
      "dog 0\n"
     ]
    }
   ],
   "source": [
    "func(doc0,doc1,CountVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird 0.004015025079257322\n",
      "cat 0.00138206928601671\n",
      "bun -1.67582883906503e-05\n",
      "dog -0.0011059905945811823\n"
     ]
    }
   ],
   "source": [
    "func(doc0,doc1,TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* \"bird\" stands out strongly\n",
    "* \"cat\" and \"dog\" are similar in absolute value\n",
    "* \"bun\" is the least significant token\n",
    "\n",
    "What about including 2-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bun bird 0.002843178755055885\n",
      "bird 0.002843178755055885\n",
      "cat cat 0.0012384810301358405\n",
      "cat 0.0009769930065187133\n",
      "bun bun 0.0001180047417965735\n",
      "bun -0.0001434832818205667\n",
      "dog bun -0.00026148802361712674\n",
      "cat dog -0.00026148802361712674\n",
      "dog dog -0.0005229760472342535\n",
      "dog -0.0007844640708513807\n"
     ]
    }
   ],
   "source": [
    "func(doc0,doc1,TfidfVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's impossible to read. Let's build better formatting into our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(doc0,doc1,vectorizer):\n",
    "    tf = vectorizer.fit_transform([doc0,doc1])\n",
    "    tfa = tf.toarray()\n",
    "    tup = zip(vectorizer.get_feature_names(),tfa[0] - tfa[1])\n",
    "    \n",
    "    # print \n",
    "    max_token_length = 0\n",
    "    output_tuples = list(reversed(sorted(tup,key=operator.itemgetter(1))))[:10]\n",
    "\n",
    "    for token,score in output_tuples:\n",
    "        if max_token_length < len(token):\n",
    "            max_token_length = len(token)\n",
    "    for token,score in output_tuples:\n",
    "        print(f\"{token:{max_token_length}s} {score:.3e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bun bird 2.843e-03\n",
      "bird     2.843e-03\n",
      "cat cat  1.238e-03\n",
      "cat      9.770e-04\n",
      "bun bun  1.180e-04\n",
      "bun      -1.435e-04\n",
      "dog bun  -2.615e-04\n",
      "cat dog  -2.615e-04\n",
      "dog dog  -5.230e-04\n",
      "dog      -7.845e-04\n"
     ]
    }
   ],
   "source": [
    "func(doc0,doc1,TfidfVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* grams with \"bird\" still stand out\n",
    "* scores are getting hard to interpret\n",
    "\n",
    "Let's get some real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from tweet_parser.tweet import Tweet\n",
    "from searchtweets import (ResultStream,\n",
    "                           collect_results,\n",
    "                           gen_rule_payload,\n",
    "                           load_credentials)\n",
    "\n",
    "search_args = load_credentials(filename=\"~/.twitter_keys.yaml\",\n",
    "                               account_type=\"enterprise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pats_rule = \"#patriots OR @patriots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eagles_rule = \"#eagles OR @eagles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date=\"2018-01-28\"\n",
    "to_date=\"2018-01-29\"\n",
    "max_results = 3000\n",
    "\n",
    "pats_rule = gen_rule_payload(_pats_rule,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        )\n",
    "eagles_rule = gen_rule_payload(_eagles_rule,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_results_list = collect_results(eagles_rule, \n",
    "                               max_results=max_results, \n",
    "                               result_stream_args=search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats_results_list = collect_results(pats_rule, \n",
    "                               max_results=max_results, \n",
    "                               result_stream_args=search_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join all tweet bodies in a corpus into one space-delimited document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_body_text = [tweet['body'] for tweet in eagles_results_list]\n",
    "eagles_doc = ' '.join(eagles_body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats_body_text = [tweet['body'] for tweet in pats_results_list]\n",
    "pats_doc = ' '.join(pats_body_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data (AS YOU ALWAYS SHOULD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @JClarkNBCS: #Eagles team hotel \\n\\n#FlyEaglesFly \\n#SuperBowl https://t.co/B0ZUh4Kt5B',\n",
       " '@JC1053 @Eagles Don’t forget to mention the cowgirls haven’t won a playoff game since 1996.',\n",
       " 'RT @JeffSkversky: #Eagles QB Nick Foles today - 1 week before the Super Bowl:\\n\\n\"I feel really good right now. I feel calm, excited, this is…',\n",
       " 'RT @JNels: The Wireless network is ready for the @Patriots and @Eagles at @NFL @MNSuperBowl2018 @SuperBowl ... @verizon boosted capacity 10…',\n",
       " '@BrunaDusi @NFLBrasil @Eagles @Patriots Mas n gosto deles, n vou torcer para eles',\n",
       " \"RT @6abc: And they're off! #Eagles are on their way to #SuperBowl LII https://t.co/jJZzNkaQa8 https://t.co/FJVNXlrCis\",\n",
       " 'RT @JClarkNBCS: #Eagles fight song as buses arrive at team hotel\\n#FlyEaglesFly \\n#SuperBowl https://t.co/cIWtv8CqqA',\n",
       " 'RT @Eagles: One week.\\n\\n#SBLII | #FlyEaglesFly https://t.co/3iMOwEfvjI',\n",
       " 'RT @RPABreaks: We are approaching #SuperBowl next Sunday @Patriots vs @Eagles I will be giving this card away via tweetdraw. To be eligible…',\n",
       " 'RT @bucksctylib: Got @Eagles fever? We have books and DVDs for fans of all ages!  #FlyEaglesFly https://t.co/dU1XpBrqW8 https://t.co/w9bmc6…']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eagles_body_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew...this is gonna take some cleaning.\n",
    "\n",
    "Let's start with a tokenizer and a stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    stop_words=stopwords,\n",
    "    ngram_range=(1,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 10 1- and 2-grams for the Eagles corpus/document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @eagles                            3.134e-01\n",
      "@eagles                               2.961e-01\n",
      "#flyeaglesfly                         2.444e-01\n",
      "#sblii #flyeaglesfly                  1.964e-01\n",
      "#sblii                                1.456e-01\n",
      "@eagles one                           1.318e-01\n",
      "week #sblii                           1.293e-01\n",
      "https://t.co/3imowefvji               1.293e-01\n",
      "#flyeaglesfly https://t.co/3imowefvji 1.293e-01\n",
      "’ #sblii                              1.098e-01\n"
     ]
    }
   ],
   "source": [
    "func(eagles_doc,pats_doc,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the ability to specify `n` in top-`n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_docs(doc0,doc1,vectorizer,n_to_display=10):\n",
    "    tfm_sparse = vectorizer.fit_transform([doc0,doc1])\n",
    "    tfm = tfm_sparse.toarray()\n",
    "    tup = zip(vectorizer.get_feature_names(),tfm[0] - tfm[1])\n",
    "    \n",
    "    # print \n",
    "    max_token_length = 0\n",
    "    output_tuples = list(reversed(sorted(tup,key=operator.itemgetter(1))))[:n_to_display]\n",
    "\n",
    "    for token,score in output_tuples:\n",
    "        if max_token_length < len(token):\n",
    "            max_token_length = len(token)\n",
    "    for token,score in output_tuples:\n",
    "        print(f\"{token:{max_token_length}s} {score:.3e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @eagles                            3.134e-01\n",
      "@eagles                               2.961e-01\n",
      "#flyeaglesfly                         2.444e-01\n",
      "#sblii #flyeaglesfly                  1.964e-01\n",
      "#sblii                                1.456e-01\n",
      "@eagles one                           1.318e-01\n",
      "week #sblii                           1.293e-01\n",
      "https://t.co/3imowefvji               1.293e-01\n",
      "#flyeaglesfly https://t.co/3imowefvji 1.293e-01\n",
      "’ #sblii                              1.098e-01\n",
      "https://t.co/uryvv4dxhv               1.092e-01\n",
      "#flyeaglesfly https://t.co/uryvv4dxhv 1.092e-01\n",
      "#eagles                               1.048e-01\n",
      "https://t.co/3imowefvji rt            1.030e-01\n",
      "https://t.co/uryvv4dxhv rt            8.232e-02\n",
      "@eagles ’                             7.867e-02\n",
      "’                                     7.201e-02\n",
      "one week                              5.223e-02\n",
      "one                                   5.189e-02\n",
      "looks back                            2.942e-02\n",
      "season                                2.834e-02\n",
      "unscripted looks                      2.800e-02\n",
      "unscripted                            2.800e-02\n",
      "two unscripted                        2.800e-02\n",
      "start #eagles                         2.800e-02\n",
      "season including                      2.800e-02\n",
      "back start                            2.800e-02\n",
      "2017 regular                          2.800e-02\n",
      "#eagles 2017                          2.800e-02\n",
      "including memorable                   2.772e-02\n"
     ]
    }
   ],
   "source": [
    "compare_docs(eagles_doc,pats_doc,vectorizer,n_to_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@patriots                  3.641e-01\n",
      "#patriots                  2.046e-01\n",
      "rt @patriots               1.623e-01\n",
      "…                          1.120e-01\n",
      "amendola                   7.496e-02\n",
      "​                          7.416e-02\n",
      "danny                      7.393e-02\n",
      "reasons                    7.256e-02\n",
      "reasons call               7.188e-02\n",
      "playoff amendola           7.188e-02\n",
      "lots reasons               7.188e-02\n",
      "https://t.co/q7cank6hna    7.188e-02\n",
      "danny playoff              7.188e-02\n",
      "call danny                 7.188e-02\n",
      "amendola 5                 7.188e-02\n",
      "@patriots lots             7.188e-02\n",
      "5 https://t.co/q7cank6hna  7.188e-02\n",
      "vs                         5.939e-02\n",
      "rt                         5.298e-02\n",
      "playoff                    5.286e-02\n",
      "5                          5.265e-02\n",
      "call                       5.196e-02\n",
      "https://t.co/q7cank6hna rt 5.169e-02\n",
      "lots                       5.119e-02\n",
      "lost                       4.733e-02\n",
      "#notdone                   4.686e-02\n",
      "#notdone network           4.655e-02\n",
      "live                       4.621e-02\n",
      "@snfonnbc                  4.251e-02\n",
      "network                    3.917e-02\n"
     ]
    }
   ],
   "source": [
    "compare_docs(pats_doc,eagles_doc,vectorizer,n_to_display=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't really evalute more sophisticated text comparison techniques without doing better filtering on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add token filtering to the TweetTokenizer\n",
    "def filter_tokens(token):\n",
    "    if len(token) < 2:\n",
    "        return False\n",
    "    if token.startswith('http'):\n",
    "        return False\n",
    "    if '’' in token:\n",
    "        return False\n",
    "    if '…' in token or '...' in token:\n",
    "        return False\n",
    "    return True\n",
    "def custom_tokenizer(doc):\n",
    "    initial_tokens = tokenizer.tokenize(doc)\n",
    "    return [token for token in initial_tokens if filter_tokens(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words=stopwords,\n",
    "    ngram_range=(1,2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @eagles           3.371e-01\n",
      "@eagles              3.140e-01\n",
      "#flyeaglesfly        2.625e-01\n",
      "#sblii #flyeaglesfly 2.112e-01\n",
      "#flyeaglesfly rt     1.755e-01\n",
      "#sblii               1.535e-01\n",
      "@eagles one          1.418e-01\n",
      "week #sblii          1.390e-01\n",
      "@eagles #sblii       1.180e-01\n",
      "#eagles              1.107e-01\n",
      "one week             5.366e-02\n",
      "one                  5.301e-02\n",
      "looks back           3.164e-02\n",
      "season               3.030e-02\n",
      "unscripted looks     3.012e-02\n",
      "unscripted           3.012e-02\n",
      "two unscripted       3.012e-02\n",
      "start #eagles        3.012e-02\n",
      "season including     3.012e-02\n",
      "back start           3.012e-02\n"
     ]
    }
   ],
   "source": [
    "compare_docs(eagles_doc,pats_doc,vectorizer,n_to_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@patriots        4.065e-01\n",
      "#patriots        2.282e-01\n",
      "rt @patriots     1.803e-01\n",
      "amendola         8.329e-02\n",
      "danny            8.215e-02\n",
      "reasons          8.063e-02\n",
      "reasons call     7.987e-02\n",
      "playoff amendola 7.987e-02\n",
      "lots reasons     7.987e-02\n",
      "danny playoff    7.987e-02\n",
      "call danny       7.987e-02\n",
      "@patriots lots   7.987e-02\n",
      "rt               7.507e-02\n",
      "vs               6.739e-02\n",
      "playoff          5.878e-02\n",
      "amendola rt      5.781e-02\n",
      "call             5.775e-02\n",
      "lots             5.688e-02\n",
      "lost             5.261e-02\n",
      "#notdone         5.218e-02\n"
     ]
    }
   ],
   "source": [
    "compare_docs(pats_doc,eagles_doc,vectorizer,n_to_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retweets makes a mess of a term frequency analysis on documents consisting of concatenated tweet bodies. Remove them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@eagles                     5.495e-01\n",
      "#eagles                     1.709e-01\n",
      "#flyeaglesfly               1.377e-01\n",
      "#flyeaglesfly te            5.409e-02\n",
      "eagles                      4.964e-02\n",
      "fans                        3.324e-02\n",
      "@thompicks                  3.245e-02\n",
      "@ryanmjb @mancitypeter      3.245e-02\n",
      "@ryanmjb                    3.245e-02\n",
      "@mancitypeter @joebennett27 3.245e-02\n",
      "@mancitypeter               3.245e-02\n",
      "@jordanwillis67             3.245e-02\n",
      "@joebennett27               3.245e-02\n",
      "@byisportsoracle @ryanmjb   3.245e-02\n",
      "@byisportsoracle            3.245e-02\n",
      "philadelphia                2.712e-02\n",
      "@thebenjohn                 2.596e-02\n",
      "@joebennett27 @eagles       2.596e-02\n",
      "@eliotevans26               2.596e-02\n",
      "#flyeaglesfly @eagles       2.596e-02\n",
      "\n",
      "\n",
      "@patriots                4.842e-01\n",
      "#patriots                2.920e-01\n",
      "#releasethememo          6.175e-02\n",
      "shop                     5.832e-02\n",
      "t-shirt shop             5.660e-02\n",
      "t-shirt                  5.660e-02\n",
      "pats t-shirt             5.660e-02\n",
      "cool pats                5.660e-02\n",
      "check cool               5.660e-02\n",
      "shop link                5.489e-02\n",
      "pats                     5.178e-02\n",
      "#fbi                     5.146e-02\n",
      "#releasethememo #fbi     4.974e-02\n",
      "#russia                  4.460e-02\n",
      "#fbi #russia             4.460e-02\n",
      "#gopats te               4.288e-02\n",
      "#gopats                  4.197e-02\n",
      "link #newenglandpatriots 4.117e-02\n",
      "@patriots check          4.117e-02\n",
      "check                    3.990e-02\n"
     ]
    }
   ],
   "source": [
    "eagles_body_text_noRT = [tweet['body'] for tweet in eagles_results_list if tweet['verb'] == 'post']\n",
    "eagles_doc_noRT = ' '.join(eagles_body_text_noRT)\n",
    "\n",
    "pats_body_text_noRT = [tweet['body'] for tweet in pats_results_list if tweet['verb'] == 'post']\n",
    "pats_doc_noRT = ' '.join(pats_body_text_noRT)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words=stopwords,\n",
    "    ngram_range=(1,2),\n",
    ")\n",
    "\n",
    "compare_docs(eagles_doc_noRT,pats_doc_noRT,vectorizer,n_to_display=20)\n",
    "print(\"\\n\")\n",
    "compare_docs(pats_doc_noRT,eagles_doc_noRT,vectorizer,n_to_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, now we have clear evidence of the political notion of the \"#patriots\" clause in our rule. Let's simplfy things by removing the hashtags from the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pats_rule = \"@patriots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eagles_rule = \"@eagles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date=\"2018-01-28\"\n",
    "to_date=\"2018-01-29\"\n",
    "max_results = 20000\n",
    "\n",
    "pats_rule = gen_rule_payload(_pats_rule,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        )\n",
    "eagles_rule = gen_rule_payload(_eagles_rule,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_results_list = collect_results(eagles_rule, \n",
    "                               max_results=max_results, \n",
    "                               result_stream_args=search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats_results_list = collect_results(pats_rule, \n",
    "                               max_results=max_results, \n",
    "                               result_stream_args=search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@eagles                     7.461e-01\n",
      "#flyeaglesfly               7.819e-02\n",
      "eagles                      5.846e-02\n",
      "@greengoblin                3.692e-02\n",
      "@ike58reese                 3.536e-02\n",
      "@nbcphiladelphia            3.423e-02\n",
      "fans                        3.192e-02\n",
      "@lanejohnson65              2.725e-02\n",
      "@malcolmjenkins @eagles     2.567e-02\n",
      "@eagles @eagles             2.538e-02\n",
      "@joel9one                   2.514e-02\n",
      "@greengoblin @lanejohnson65 2.481e-02\n",
      "@jclarknbcs                 2.412e-02\n",
      "@malcolmjenkins             2.371e-02\n",
      "@nbcsphilly                 2.288e-02\n",
      "fly                         2.110e-02\n",
      "@joel9one @ike58reese       2.077e-02\n",
      "@ike58reese @malcolmjenkins 2.077e-02\n",
      "#eagles                     1.979e-02\n",
      "@nbcphiladelphia @eagles    1.940e-02\n",
      "\n",
      "\n",
      "@patriots           7.699e-01\n",
      "@nfl                8.946e-02\n",
      "@nfl @patriots      5.809e-02\n",
      "@nfl @jaguars       3.519e-02\n",
      "@jaguars            3.484e-02\n",
      "@jaguars @patriots  3.045e-02\n",
      "@patriots @patriots 2.953e-02\n",
      "@thehall            2.549e-02\n",
      "brady               2.484e-02\n",
      "pats                2.458e-02\n",
      "#notdone            2.273e-02\n",
      "@eagles @patriots   2.196e-02\n",
      "tom                 2.169e-02\n",
      "@patriots happy     2.154e-02\n",
      "@mark15_11          1.975e-02\n",
      "#gopats             1.960e-02\n",
      "happy               1.950e-02\n",
      "@tallguy2436        1.939e-02\n",
      "@hoperugg           1.939e-02\n",
      "@bossportsextra     1.939e-02\n"
     ]
    }
   ],
   "source": [
    "eagles_body_text_noRT = [tweet['body'] for tweet in eagles_results_list if tweet['verb'] == 'post']\n",
    "eagles_doc_noRT = ' '.join(eagles_body_text_noRT)\n",
    "\n",
    "pats_body_text_noRT = [tweet['body'] for tweet in pats_results_list if tweet['verb'] == 'post']\n",
    "pats_doc_noRT = ' '.join(pats_body_text_noRT)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words=stopwords,\n",
    "    ngram_range=(1,2),\n",
    ")\n",
    "\n",
    "compare_docs(eagles_doc_noRT,pats_doc_noRT,vectorizer,n_to_display=20)\n",
    "print(\"\\n\")\n",
    "compare_docs(pats_doc_noRT,eagles_doc_noRT,vectorizer,n_to_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things we could do:\n",
    "* vectorize tweets as documents, and summarize or aggregate the coeeficients \n",
    "* select tokens for which the mean coefficient within a corpus is zero\n",
    "* look at the difference in mean coefficient\n",
    "\n",
    "Let's start by going back to simple corpora, and account for individual docs this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus0 = [\"cat\",\"cat dog\"]\n",
    "corpus1 = [\"bun\",\"dog\",\"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic unigram vectorizer with Twitter-specific tokenization and stopwords\n",
    "vectorizer = CountVectorizer(\n",
    "                    tokenizer=custom_tokenizer,\n",
    "                    stop_words=stopwords,\n",
    "                    ngram_range=(1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bun' 'cat' 'dog']\n",
      "[[0 1 0]\n",
      " [0 1 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# get the term-frequency matrix\n",
    "m = vectorizer.fit_transform(corpus0+corpus1)\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print(vocab)\n",
    "\n",
    "m = m.toarray()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# get TF matrices for each corpus\n",
    "corpus0_indices = range(len(corpus0))\n",
    "corpus1_indices = range(len(corpus0),len(corpus0)+len(corpus1))\n",
    "m0 = m[corpus0_indices,:]\n",
    "m1 = m[corpus1_indices,:]\n",
    "print(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "# calculate the average term frequency within each corpus\n",
    "c0_means = np.mean(m0,axis=0)\n",
    "c1_means = np.mean(m1,axis=0)\n",
    "print(c0_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bun']\n"
     ]
    }
   ],
   "source": [
    "# calculate the indices of the distinct tokens, which only occur in a single corpus\n",
    "distinct_indices = c0_means * c1_means == 0\n",
    "print(vocab[distinct_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(m[:, np.invert(distinct_indices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and identify the corpora\n",
    "docs = eagles_body_text_noRT + pats_body_text_noRT\n",
    "eagles_indices = range(len(eagles_body_text_noRT))\n",
    "pats_indices = range(len(eagles_body_text_noRT),len(eagles_body_text_noRT) + len(pats_body_text_noRT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a single vectorizer because we care about the joint vocabulary\n",
    "vectorizer = CountVectorizer(\n",
    "                    tokenizer=custom_tokenizer,\n",
    "                    stop_words=stopwords,\n",
    "                    ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "dtm = vectorizer.fit_transform(docs).toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "eagles_dtm = dtm[eagles_indices, :]\n",
    "pats_dtm = dtm[pats_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the average coefficient for each vocab element, for each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for every token in the vocab; rows for tweets in the corpus\n",
    "eagles_means = np.mean(eagles_dtm,axis=0)\n",
    "pats_means = np.mean(pats_dtm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by looking for _distinct_ tokens, which only exist in one corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices for any column with zero mean in either corpus\n",
    "distinctive_indices = eagles_means * pats_means == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8073 distinct tokens out of 11759\n"
     ]
    }
   ],
   "source": [
    "print(str(np.count_nonzero(distinctive_indices)) + \" distinct tokens out of \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "eagles_ranking = np.argsort(eagles_means[distinctive_indices])[::-1]\n",
    "pats_ranking = np.argsort(pats_means[distinctive_indices])[::-1]\n",
    "total_ranking = np.argsort(eagles_means[distinctive_indices] + pats_means[distinctive_indices])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@greengoblin', '@thehall', '@mark15_11', ..., 'craig', 'crazies',\n",
       "       '##pitt'], dtype='<U52')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[distinctive_indices][total_ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top distinct Eagles tokens by average term count in Eagles corpus\n",
      "@greengoblin                   0.0269\n",
      "@joslewis                      0.0133\n",
      "@johnkincade                   0.00968\n",
      "@johngaudreau03                0.00884\n",
      "@nhlflames                     0.00673\n",
      "@airbnb                        0.00673\n",
      "@themightyerock                0.00652\n",
      "@torreysmithwr                 0.00589\n",
      "@smittybarstool                0.00589\n",
      "@treyburton8                   0.00568\n"
     ]
    }
   ],
   "source": [
    "print(\"Top distinct Eagles tokens by average term count in Eagles corpus\")\n",
    "for token in vocab[distinctive_indices][eagles_ranking][:10]:\n",
    "    print_str = f\"{token:30s} {eagles_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "    print(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top distinct Patriots tokens by average term count in Patriots corpus\n",
      "@thehall                       0.0186\n",
      "@mark15_11                     0.0144\n",
      "@tallguy2436                   0.0142\n",
      "@bossportsextra                0.0142\n",
      "@hoperugg                      0.0142\n",
      "@lesliej23                     0.0139\n",
      "@raider_35_24                  0.0137\n",
      "@aolisn87                      0.0131\n",
      "@jcapmany1231                  0.0126\n",
      "@sarahlee626                   0.0126\n"
     ]
    }
   ],
   "source": [
    "print(\"Top distinct Patriots tokens by average term count in Patriots corpus\")\n",
    "for token in vocab[distinctive_indices][pats_ranking][:10]:\n",
    "    print_str = f\"{token:30s} {pats_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "    print(print_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this change if we account for inverse document frequency?\n",
    "\n",
    "Let's build a function and encapsulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_corpora(corpus0,corpus1,vectorizer,n_to_display=10):\n",
    "    corpus0_indices = range(len(corpus0))\n",
    "    corpus1_indices = range(len(corpus0), len(corpus0) + len(corpus1))\n",
    "    m_sparse = vectorizer.fit_transform(corpus0 + corpus1)\n",
    "    m = m_sparse.toarray()\n",
    "\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    m_corpus0 = m[corpus0_indices,:]\n",
    "    m_corpus1 = m[corpus1_indices,:]\n",
    "    \n",
    "    corpus0_means = np.mean(m_corpus0,axis=0)\n",
    "    corpus1_means = np.mean(m_corpus1,axis=0)\n",
    "    \n",
    "    distinctive_indices = corpus0_means * corpus1_means == 0\n",
    "    print(str(np.count_nonzero(distinctive_indices)) + \" distinct tokens out of \" + str(len(vocab)) + '\\n')    \n",
    "    \n",
    "    corpus0_ranking = np.argsort(corpus0_means[distinctive_indices])[::-1]\n",
    "    corpus1_ranking = np.argsort(corpus1_means[distinctive_indices])[::-1]\n",
    "\n",
    "    print(\"Top distinct tokens from corpus0 by average term count in corpus\")\n",
    "    for token in vocab[distinctive_indices][corpus0_ranking][:n_to_display]:\n",
    "        print_str = f\"{token:30s} {corpus0_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "        print(print_str)\n",
    "    print()\n",
    "    print(\"Top distinct tokens from corpus1 by average term count in corpus\")\n",
    "    for token in vocab[distinctive_indices][corpus1_ranking][:n_to_display]:\n",
    "        print_str = f\"{token:30s} {corpus1_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "        print(print_str)    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    tup = zip(vectorizer.get_feature_names(),tfm[0] - tfm[1])\n",
    "    \n",
    "    # print \n",
    "    max_token_length = 0\n",
    "    output_tuples = list(reversed(sorted(tup,key=operator.itemgetter(1))))[:n_to_display]\n",
    "\n",
    "    for token,score in output_tuples:\n",
    "        if max_token_length < len(token):\n",
    "            max_token_length = len(token)\n",
    "    for token,score in output_tuples:\n",
    "        print(f\"{token:{max_token_length}s} {score:.3e}\") \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8073 distinct tokens out of 11759\n",
      "\n",
      "Top distinct tokens from corpus0 by average term count in corpus\n",
      "@greengoblin                   0.0269\n",
      "@joslewis                      0.0133\n",
      "@johnkincade                   0.00968\n",
      "@johngaudreau03                0.00884\n",
      "@nhlflames                     0.00673\n",
      "@airbnb                        0.00673\n",
      "@themightyerock                0.00652\n",
      "@torreysmithwr                 0.00589\n",
      "@smittybarstool                0.00589\n",
      "@treyburton8                   0.00568\n",
      "\n",
      "Top distinct tokens from corpus1 by average term count in corpus\n",
      "@thehall                       0.0186\n",
      "@mark15_11                     0.0144\n",
      "@tallguy2436                   0.0142\n",
      "@bossportsextra                0.0142\n",
      "@hoperugg                      0.0142\n",
      "@lesliej23                     0.0139\n",
      "@raider_35_24                  0.0137\n",
      "@aolisn87                      0.0131\n",
      "@jcapmany1231                  0.0126\n",
      "@sarahlee626                   0.0126\n"
     ]
    }
   ],
   "source": [
    "#vectorizer = TfidfVectorizer(\n",
    "vectorizer = CountVectorizer(\n",
    "                    tokenizer=custom_tokenizer,\n",
    "                    stop_words=stopwords,\n",
    "                    ngram_range=(1,1)\n",
    ")\n",
    "compare_corpora(eagles_body_text_noRT,pats_body_text_noRT,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the distrinctive tokens and look at the maximum _difference_ in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_corpora(corpus0,corpus1,vectorizer,n_to_display=10):\n",
    "    corpus0_indices = range(len(corpus0))\n",
    "    corpus1_indices = range(len(corpus0), len(corpus0) + len(corpus1))\n",
    "    m_sparse = vectorizer.fit_transform(corpus0 + corpus1)\n",
    "    m = m_sparse.toarray()\n",
    "\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    m_corpus0 = m[corpus0_indices,:]\n",
    "    m_corpus1 = m[corpus1_indices,:]\n",
    "    \n",
    "    corpus0_means = np.mean(m_corpus0,axis=0)\n",
    "    corpus1_means = np.mean(m_corpus1,axis=0)\n",
    "    \n",
    "    distinctive_indices = corpus0_means * corpus1_means == 0\n",
    "    print(str(np.count_nonzero(distinctive_indices)) + \" distinct tokens out of \" + str(len(vocab)) + '\\n')    \n",
    "    \n",
    "    corpus0_ranking = np.argsort(corpus0_means[distinctive_indices])[::-1]\n",
    "    corpus1_ranking = np.argsort(corpus1_means[distinctive_indices])[::-1]\n",
    "\n",
    "    print(\"Top distinct tokens from corpus0 by average term count in corpus\")\n",
    "    for token in vocab[distinctive_indices][corpus0_ranking][:n_to_display]:\n",
    "        print_str = f\"{token:30s} {corpus0_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "        print(print_str)\n",
    "    print()\n",
    "    print(\"Top distinct tokens from corpus1 by average term count in corpus\")\n",
    "    for token in vocab[distinctive_indices][corpus1_ranking][:n_to_display]:\n",
    "        print_str = f\"{token:30s} {corpus1_means[vectorizer.vocabulary_[token]]:.3g}\"\n",
    "        print(print_str)    \n",
    "    \n",
    "    # remove distinct tokens\n",
    "    m = m[:, np.invert(distinctive_indices)]\n",
    "    vocab = vocab[np.invert(distinctive_indices)]\n",
    "    \n",
    "    corpus0_means = np.mean(m_corpus0,axis=0)\n",
    "    corpus1_means = np.mean(m_corpus1,axis=0)\n",
    "    keyness = corpus0_means - corpus1_means\n",
    "    \n",
    "    \"\"\"\n",
    "    tup = zip(vectorizer.get_feature_names(),tfm[0] - tfm[1])\n",
    "    \n",
    "    # print \n",
    "    max_token_length = 0\n",
    "    output_tuples = list(reversed(sorted(tup,key=operator.itemgetter(1))))[:n_to_display]\n",
    "\n",
    "    for token,score in output_tuples:\n",
    "        if max_token_length < len(token):\n",
    "            max_token_length = len(token)\n",
    "    for token,score in output_tuples:\n",
    "        print(f\"{token:{max_token_length}s} {score:.3e}\") \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8073 distinct tokens out of 11759\n",
      "\n",
      "Top distinct tokens from corpus0 by average term count in corpus\n",
      "@greengoblin                   0.0269\n",
      "@joslewis                      0.0133\n",
      "@johnkincade                   0.00968\n",
      "@johngaudreau03                0.00884\n",
      "@nhlflames                     0.00673\n",
      "@airbnb                        0.00673\n",
      "@themightyerock                0.00652\n",
      "@torreysmithwr                 0.00589\n",
      "@smittybarstool                0.00589\n",
      "@treyburton8                   0.00568\n",
      "\n",
      "Top distinct tokens from corpus1 by average term count in corpus\n",
      "@thehall                       0.0186\n",
      "@mark15_11                     0.0144\n",
      "@tallguy2436                   0.0142\n",
      "@bossportsextra                0.0142\n",
      "@hoperugg                      0.0142\n",
      "@lesliej23                     0.0139\n",
      "@raider_35_24                  0.0137\n",
      "@aolisn87                      0.0131\n",
      "@jcapmany1231                  0.0126\n",
      "@sarahlee626                   0.0126\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "                    tokenizer=custom_tokenizer,\n",
    "                    stop_words=stopwords,\n",
    "                    ngram_range=(1,1)\n",
    ")\n",
    "compare_corpora(eagles_body_text_noRT,pats_body_text_noRT,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinctive_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8559, 11759)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([False, True, False])\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
